# -*- coding: utf-8 -*-
"""Ground Truth Generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CzMhRB1IOn7yj-hW35N_KzZQRFxZX3tc

# Ground Truth Generation
"""
import os
# badlad_json_path = '/content/drive/MyDrive/BANGLAOCR/badlad-test-coco.json'
# json_dir = '/content/drive/MyDrive/BANGLAOCR/jsons'

base_path = '/mnt/hdd/jawaril/full_pipeline/Evaluation_Metric'

badlad_coco_path = r'data/badlad-test-coco.json'
word_annotation_directory = r'data/eval_218/word_annotations'
image_dir = r'data/eval_218/images'
# Replace 'output.csv' with the desired CSV file name
output_path = 'data/eval_218/reconstruction_v2_gt.pkl'

badlad_json_path = os.path.join(base_path, badlad_coco_path)
json_dir = os.path.join(base_path, word_annotation_directory)
output_file = os.path.join(base_path, output_path)
image_path = os.path.join(base_path, image_dir)


import cv2
import json
import os
# from google.colab.patches import cv2_imshow

# from google.colab import drive
# drive.mount('/content/drive')

def get_centroids(x,y,width, height):
  centroid_x = x + (width / 2)
  centroid_y = y + (height / 2)
  return centroid_x,centroid_y

def all_words_from_json(word_json_path, image_name):
  with open(word_json_path,"r",encoding='utf8') as f:
          word_json=json.load(f)

  # single_word = word_json['00fb93d5-7c67-4851-ad08-f23ed2159467 (1).png643395']['regions'][120]
  all_words = []
  # j=0
  # keys = list(word_json.keys())
  # if len(keys)>1:
  #   j=2
  size = os.path.getsize(os.path.join(image_path, image_name))
  json_key = image_name + str(size)
  
  for i in range(len(word_json[json_key]['regions'])):
    single_word = word_json[json_key]['regions'][i]
    if(single_word['shape_attributes']['name']=='rect'):
      x = single_word['shape_attributes']['x']
      y = single_word['shape_attributes']['y']
      width = single_word['shape_attributes']['width']
      height = single_word['shape_attributes']['height']
      word = single_word['region_attributes']['name']
      # all_words.append([word, x, y, width+x, height+y])
      all_words.append([word, x, y, width, height])
      # print(x, y, width+x, height+y, word)

  return all_words

def sort_words_by_position(words_list):
    # Sort by y_min
    words_list.sort(key=lambda word: (word[6], word[5]))

    # Group words with the same y_min
    grouped_words = []
    current_group = []
    prev_word = None

    for word in words_list:
        if prev_word is None or word[6]-5<= prev_word[6] <= word[6]+5:
            current_group.append(word)
        else:
            grouped_words.append(current_group)
            current_group = [word]
        prev_word = word

    grouped_words.append(current_group)

    # Sort words within groups based on x_min
    for group in grouped_words:
        group.sort(key=lambda word: word[5])

    # Flatten the grouped words back to a single list
    sorted_words = [word for group in grouped_words for word in group]

    return sorted_words


# sorted_words = sort_words_by_position(word_in_pg)

# for word in sorted_words:
#     print(word[0], "" , end='')  # Print the word itself

# import cv2
# import json
# from google.colab.patches import cv2_imshow
# image_path = '/content/drive/MyDrive/Bangla OCR test data/Test Data Annotation Group/Marshia /00e1a7f8-5bac-46f5-8348-cfe7cd634ef9.png'

# word_json_path= '/content/drive/MyDrive/Bangla OCR test data/Test Data Annotation Group/Marshia /00e1a7f8-5bac-46f5-8348-cfe7cd634ef9.json'
# image_name = '00e1a7f8-5bac-46f5-8348-cfe7cd634ef9.png'
# image = cv2.imread(image_path)
# # cv2_imshow(image)

import json

ascii_letters = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'
digits = '0123456789'

englist_vocab = ascii_letters + digits

from bnunicodenormalizer import Normalizer
NORM=Normalizer()

def gt_word_text_filter(gt_word_text):
    # remove start/end spaces,newlines,tabs
    gt_word_text = gt_word_text.strip()
    
    for ch in gt_word_text:
        # if ch not in bn_ocr_vocab:
        if ch in englist_vocab:
            return False, gt_word_text
    
    gt_word_text_frags = gt_word_text.split(' ')
    gt_word_text_frags = [NORM(frag)["normalized"] for frag in gt_word_text_frags]
    # import pdb; pdb.set_trace()
    gt_word_text_frags = list(filter(lambda x: x is not None, gt_word_text_frags))
    gt_word_text = ' '.join(gt_word_text_frags)
  
    
    return True, gt_word_text


def gt_single_image(word_json_path, badlad_json, image_name):

  result_df = {}
  result_df[image_name]={}

#   word_json_path= '/content/drive/MyDrive/Bangla OCR test data/Test Data Annotation Group/Marshia /00e1a7f8-5bac-46f5-8348-cfe7cd634ef9.json'
#   badlad_json_path = '/content/drive/MyDrive/BANGLAOCR/badlad-test-coco.json'
#   with open(badlad_json_path,"r",encoding='utf8') as f:
#           badlad_json=json.load(f)

  image_id = -1
  for img in badlad_json["images"]:
    if img["file_name"]==image_name:
      image_id = img["id"]
    #   print("ok")
      break

  all_paragraph_textbox_seg = []
  for seg in badlad_json['annotations']:
    if seg['image_id'] == image_id and seg['category_id']<2:
    #   print(seg['category_id'])
      all_paragraph_textbox_seg.append(seg)
  # [test] = all_paragraph_textbox_seg[0]['segmentation']

  all_words = all_words_from_json(word_json_path, image_name)


  for i in range(len(all_paragraph_textbox_seg)):
    # import pdb; pdb.set_trace()
    check = all_paragraph_textbox_seg[i]['bbox']
    word_in_pg = []
    for word in all_words:

      cent_x, cent_y = get_centroids(word[1], word[2], word[3], word[4])
      if cent_x>= check[0] and cent_x<= check[0]+check[2] and cent_y>= check[1] and cent_y<= check[1]+check[3]:
        word.append(cent_x)
        word.append(cent_y)
        word_in_pg.append(word)

    sorted_words = sort_words_by_position(word_in_pg)

    word_list= []
    for word in sorted_words:
        # print(word[0], "" , end='')
        status, word_final = gt_word_text_filter(word[0])
        if status:
          word_list.append(word[0])

    sorted_str = ' '.join(word_list)
    all_paragraph_textbox_seg[i]['text'] = sorted_str
    # all_paragraph_textbox_seg[i]['image_name'] = image_name
    # result_df[image_name].update({i:sorted_str})
    
  result = {
    image_name: all_paragraph_textbox_seg,
  }
    
  # return result_df, all_paragraph_textbox_seg
  return result



# for seg in all_paragraph_textbox_seg:
#   [coord] = seg['segmentation']
#   x_min = int(min(coord[0],coord[6]))
#   y_min = int(min(coord[1],coord[3]))
#   x_max = int(max(coord[2],coord[4]))
#   y_max = int(max(coord[5],coord[7]))

#   crop_img = image[y_min:y_max, x_min:x_max]
#   cv2_imshow(crop_img)
#   print(x_min, y_min, x_max, y_max)

import json
from json import loads

with open(badlad_json_path,"r",encoding='utf8') as f:
        badlad_json=json.load(f)

import json
from json import loads
jsons_list = os.listdir(json_dir)
result = {}
error_file = []
outputs = {}
for jsons in jsons_list:
    image_name = jsons[:-5]+'.png'
    word_json_path = os.path.join(json_dir,jsons)
    # print(word_json_path)

    # with open(word_json_path,"r",encoding='utf8') as fl:
    #     word_json=json.load(fl)
    # try:
    if 1:
        single_image_gt = gt_single_image(word_json_path, badlad_json, image_name)
        # result.update(single_image_gt)
        outputs.update(single_image_gt)
        
        # print('done', jsons)
    # except:
        # print('Error', jsons)
        # error_file.append(image_name)

len(result)

# result
# import pandas as pd
# df = pd.DataFrame(result)

# Write the DataFrame to a CSV file
# df.to_csv(output_file, index=False)
# with open(output_file, 'w', encoding='utf-8') as output:
#   json.dump(outputs, output)

import pickle

with open(output_file, 'wb') as output:
  pickle.dump(outputs, output)
  

# df.head()

# img = '/content/drive/MyDrive/Bangla OCR test data/annotated_data/source/images/'
# for er in error_file:
#     img_path = img+er
#     image = cv2.imread(img_path)
#     cv2_imshow(image)

