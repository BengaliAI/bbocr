{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY0WMSQJPaX-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEzLk7f7QDga"
      },
      "outputs": [],
      "source": [
        "!nvcc --version\n",
        "import torch, torchvision,numpy\n",
        "print(torch.__version__, torch.cuda.is_available(), numpy.__version__) #1.13.1 True 1.23.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE6iwnGGQkDA"
      },
      "outputs": [],
      "source": [
        "!pip install -q ultralytics\n",
        "!pip install -q -U layoutparser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UPlcjg5Rf4J"
      },
      "outputs": [],
      "source": [
        "import layoutparser as lp\n",
        "import cv2\n",
        "import yaml\n",
        "import io\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVyq1Fm0y56_"
      },
      "outputs": [],
      "source": [
        "# for ocr\n",
        "!wget https://paddle-wheel.bj.bcebos.com/2.3.0/linux/linux-gpu-cuda11.1-cudnn8.1-mkl-gcc8.2-avx/paddlepaddle_gpu-2.3.0.post111-cp38-cp38-linux_x86_64.whl\n",
        "!pip install -q paddlepaddle_gpu-2.3.0.post111-cp38-cp38-linux_x86_64.whl\n",
        "!pip uninstall -q protobuf\n",
        "!pip install -q --no-binary protobuf protobuf #==3.18.0\n",
        "!pip install -q opencv-python==4.6.0.66\n",
        "!pip install -q shapely==1.8.2\n",
        "!pip install -q pyclipper==1.3.0.post3\n",
        "!pip install -q scikit-image==0.19.3\n",
        "!pip install -q imgaug==0.4.0\n",
        "!pip install -q lmdb==1.3.0\n",
        "!pip install -q tqdm==4.64.0\n",
        "!pip install -q attrdict==2.0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA5mG7br4rBZ"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install -q gcc-7\n",
        "\n",
        "!python -m pip install -q paddlepaddle-gpu==2.3.0.post111 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html\n",
        "# !pip install paddleocr==2.5.0.3\n",
        "\n",
        "!pip install -q paddlepaddle-gpu -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
        "\n",
        "# !pip install -q paddlepaddle -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
        "\n",
        "!pip install -q \"paddleocr>=2.0.1\" # Recommend to use version 2.0.1+\n",
        "# !pip install -q dist/paddleocr-2.0.1-py3-none-any.whl # x.x.x is the version of paddleocr\n",
        "\n",
        "!pip install -q paddleocr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/lib/python3.10/dist-packages/paddle/fluid/libpaddle.so"
      ],
      "metadata": {
        "id": "vkKh1k2SzDnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCq8twBJ4tTY"
      },
      "outputs": [],
      "source": [
        "from paddleocr import PaddleOCR\n",
        "import numpy as np\n",
        "import cv2\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from glob import glob\n",
        "from pprint import pprint\n",
        "import os\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh3630vS3p9w"
      },
      "outputs": [],
      "source": [
        "line=PaddleOCR(use_angle_cls=False, lang='en',use_gpu=True)\n",
        "word=PaddleOCR(use_angle_cls=False, lang='ar',use_gpu=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW4aLopgF2Ch"
      },
      "outputs": [],
      "source": [
        "!pip install -q bnunicodenormalizer\n",
        "!pip install -q onnxruntime-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM_UMiM0EctA"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "#-------------------------\n",
        "# imports\n",
        "#-------------------------\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import cv2\n",
        "from bnunicodenormalizer import Normalizer\n",
        "NORM=Normalizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lMrQfRC3-bH"
      },
      "outputs": [],
      "source": [
        "class BanglaOCR(object):\n",
        "    def __init__(self,\n",
        "                model_weights,\n",
        "                providers=['CUDAExecutionProvider'],\n",
        "                img_height=32,\n",
        "                img_width=256,\n",
        "                pos_max=40):\n",
        "        self.img_height=img_height\n",
        "        self.img_width =img_width\n",
        "        self.pos_max   =pos_max\n",
        "        self.model     =ort.InferenceSession(model_weights, providers=providers)\n",
        "        self.vocab     =[\"blank\",\"!\",\"\\\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\"=\",\">\",\"?\",\"।\",\n",
        "                        \"ঁ\",\"ং\",\"ঃ\",\"অ\",\"আ\",\"ই\",\"ঈ\",\"উ\",\"ঊ\",\"ঋ\",\"এ\",\"ঐ\",\"ও\",\"ঔ\",\n",
        "                        \"ক\",\"খ\",\"গ\",\"ঘ\",\"ঙ\",\"চ\",\"ছ\",\"জ\",\"ঝ\",\"ঞ\",\"ট\",\"ঠ\",\"ড\",\"ঢ\",\"ণ\",\"ত\",\"থ\",\"দ\",\"ধ\",\"ন\",\n",
        "                        \"প\",\"ফ\",\"ব\",\"ভ\",\"ম\",\"য\",\"র\",\"ল\",\"শ\",\"ষ\",\"স\",\"হ\",\n",
        "                        \"া\",\"ি\",\"ী\",\"ু\",\"ূ\",\"ৃ\",\"ে\",\"ৈ\",\"ো\",\"ৌ\",\"্\",\n",
        "                        \"ৎ\",\"ড়\",\"ঢ়\",\"য়\",\"০\",\"১\",\"২\",\"৩\",\"৪\",\"৫\",\"৬\",\"৭\",\"৮\",\"৯\",\"‍\",\"sep\",\"pad\"]\n",
        "\n",
        "    def process_batch(self,crops):\n",
        "        batch_img=[]\n",
        "        batch_pos=[]\n",
        "        for img in crops:\n",
        "            # correct padding\n",
        "            img,_=correctPadding(img,(self.img_height,self.img_width))\n",
        "            # normalize\n",
        "            img=img/255.0\n",
        "            # extend batch\n",
        "            img=np.expand_dims(img,axis=0)\n",
        "            batch_img.append(img)\n",
        "            # pos\n",
        "            pos=np.array([[i for i in range(self.pos_max)]])\n",
        "            batch_pos.append(pos)\n",
        "        # stack\n",
        "        img=np.vstack(batch_img)\n",
        "        img=img.astype(np.float32)\n",
        "        pos=np.vstack(batch_pos)\n",
        "        pos=pos.astype(np.float32)\n",
        "        # batch inp\n",
        "        return {\"image\":img,\"pos\":pos}\n",
        "\n",
        "    def __call__(self,crops,batch_size=32):\n",
        "        # adjust batch_size\n",
        "        if len(crops)<batch_size:\n",
        "            batch_size=len(crops)\n",
        "        texts=[]\n",
        "        for idx in range(0,len(crops),batch_size):\n",
        "            batch=crops[idx:idx+batch_size]\n",
        "            inp=self.process_batch(batch)\n",
        "            preds=self.model.run(None,inp)[0]\n",
        "            preds =np.argmax(preds,axis=-1)\n",
        "            # decoding\n",
        "            for pred in preds:\n",
        "                label=\"\"\n",
        "                for c in pred[1:]:\n",
        "                    if c!=self.vocab.index(\"sep\"):\n",
        "                        label+=self.vocab[c]\n",
        "                    else:\n",
        "                        break\n",
        "                texts.append(label)\n",
        "        texts=[NORM(text)[\"normalized\"] for text in texts]\n",
        "        texts=[text for text in texts if text is not None]\n",
        "        return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_3FEhk5Hi8o"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "# ---------------------------------------------------------\n",
        "# imports\n",
        "# ---------------------------------------------------------\n",
        "import cv2\n",
        "import numpy as np\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_GIMovzH00n"
      },
      "outputs": [],
      "source": [
        "def padWordImage(img,pad_loc,pad_dim,pad_val):\n",
        "    '''\n",
        "        pads an image with white value\n",
        "        args:\n",
        "            img     :       the image to pad\n",
        "            pad_loc :       (lr/tb) lr: left-right pad , tb=top_bottom pad\n",
        "            pad_dim :       the dimension to pad upto\n",
        "            pad_val :       the value to pad\n",
        "    '''\n",
        "\n",
        "    if pad_loc==\"lr\":\n",
        "        # shape\n",
        "        h,w,d=img.shape\n",
        "        # pad widths\n",
        "        pad_width =pad_dim-w\n",
        "        # pads\n",
        "        pad =np.ones((h,pad_width,3))*pad_val\n",
        "        # pad\n",
        "        img =np.concatenate([img,pad],axis=1)\n",
        "    else:\n",
        "        # shape\n",
        "        h,w,d=img.shape\n",
        "        # pad heights\n",
        "        if h>= pad_dim:\n",
        "            return img\n",
        "        else:\n",
        "            pad_height =pad_dim-h\n",
        "            # pads\n",
        "            pad =np.ones((pad_height,w,3))*pad_val\n",
        "            # pad\n",
        "            img =np.concatenate([img,pad],axis=0)\n",
        "    return img.astype(\"uint8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWWskKZiHrlB"
      },
      "outputs": [],
      "source": [
        "def correctPadding(img,dim,pvalue=255):\n",
        "    '''\n",
        "        corrects an image padding\n",
        "        args:\n",
        "            img     :       numpy array of single channel image\n",
        "            dim     :       tuple of desired img_height,img_width\n",
        "            pvalue  :       the value to pad\n",
        "        returns:\n",
        "            correctly padded image\n",
        "\n",
        "    '''\n",
        "    img_height,img_width=dim\n",
        "    mask=0\n",
        "    # check for pad\n",
        "    h,w,d=img.shape\n",
        "\n",
        "    w_new=int(img_height* w/h)\n",
        "    img=cv2.resize(img,(w_new,img_height))\n",
        "    h,w,d=img.shape\n",
        "\n",
        "    if w > img_width:\n",
        "        # for larger width\n",
        "        h_new= int(img_width* h/w)\n",
        "        img=cv2.resize(img,(img_width,h_new))\n",
        "        # pad\n",
        "        img=padWordImage(img,\n",
        "                     pad_loc=\"tb\",\n",
        "                     pad_dim=img_height,\n",
        "                     pad_val=pvalue)\n",
        "        mask=img_width\n",
        "\n",
        "    elif w < img_width:\n",
        "        # pad\n",
        "        img=padWordImage(img,\n",
        "                    pad_loc=\"lr\",\n",
        "                    pad_dim=img_width,\n",
        "                    pad_val=pvalue)\n",
        "        mask=w\n",
        "\n",
        "    # error avoid\n",
        "    img=cv2.resize(img,(img_width,img_height))\n",
        "\n",
        "    return img,mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1JVg6LVDqY5"
      },
      "outputs": [],
      "source": [
        "class Detector(object):\n",
        "    def __init__(self):\n",
        "        '''\n",
        "            initializes a dbnet detector model\n",
        "        '''\n",
        "        self.call_rec=\"paddle\"\n",
        "\n",
        "    def sorted_boxes(self,dt_boxes,dist=10):\n",
        "        \"\"\"\n",
        "        Sort text boxes in order from top to bottom, left to right\n",
        "        args:\n",
        "            dt_boxes(array):detected text boxes with shape [4, 2]\n",
        "        return:\n",
        "            sorted boxes(array) with shape [4, 2]\n",
        "        \"\"\"\n",
        "        num_boxes = dt_boxes.shape[0]\n",
        "        sorted_boxes = sorted(dt_boxes, key=lambda x: (x[0][1], x[0][0]))\n",
        "        _boxes = list(sorted_boxes)\n",
        "\n",
        "        for i in range(num_boxes - 1):\n",
        "            if abs(_boxes[i + 1][0][1] - _boxes[i][0][1]) < dist and (_boxes[i + 1][0][0] < _boxes[i][0][0]):\n",
        "                tmp = _boxes[i]\n",
        "                _boxes[i] = _boxes[i + 1]\n",
        "                _boxes[i + 1] = tmp\n",
        "        return _boxes\n",
        "\n",
        "    def get_rotate_crop_image(self,img, points):\n",
        "        # Use Green's theory to judge clockwise or counterclockwise\n",
        "        # author: biyanhua\n",
        "        d = 0.0\n",
        "        for index in range(-1, 3):\n",
        "            d += -0.5 * (points[index + 1][1] + points[index][1]) * (\n",
        "                        points[index + 1][0] - points[index][0])\n",
        "        if d < 0: # counterclockwise\n",
        "            tmp = np.array(points)\n",
        "            points[1], points[3] = tmp[3], tmp[1]\n",
        "\n",
        "        img_crop_width = int(\n",
        "            max(\n",
        "                np.linalg.norm(points[0] - points[1]),\n",
        "                np.linalg.norm(points[2] - points[3])))\n",
        "        img_crop_height = int(\n",
        "            max(\n",
        "                np.linalg.norm(points[0] - points[3]),\n",
        "                np.linalg.norm(points[1] - points[2])))\n",
        "        pts_std = np.float32([[0, 0], [img_crop_width, 0],\n",
        "                            [img_crop_width, img_crop_height],\n",
        "                            [0, img_crop_height]])\n",
        "        M = cv2.getPerspectiveTransform(points, pts_std)\n",
        "        dst_img = cv2.warpPerspective(\n",
        "            img,\n",
        "            M, (img_crop_width, img_crop_height),\n",
        "            borderMode=cv2.BORDER_REPLICATE,\n",
        "            flags=cv2.INTER_CUBIC)\n",
        "        dst_img_height, dst_img_width = dst_img.shape[0:2]\n",
        "        if dst_img_height * 1.0 / dst_img_width >= 1.5:\n",
        "            dst_img = np.rot90(dst_img)\n",
        "        return dst_img\n",
        "\n",
        "\n",
        "    def __call__(self,img,result):\n",
        "        '''\n",
        "            extract locations and crops\n",
        "        '''\n",
        "        boxes= np.array(result, dtype=np.float32)\n",
        "\n",
        "        # boxes=self.sorted_boxes(boxes) # This existed in the original code\n",
        "\n",
        "        crops=[]\n",
        "        for bno in range(len(boxes)):\n",
        "            tmp_box = copy.deepcopy(boxes[bno])\n",
        "            img_crop = self.get_rotate_crop_image(img,tmp_box)\n",
        "            crops.append(img_crop)\n",
        "        #mask=create_mask(img,boxes)\n",
        "        #return mask,boxes,crops\n",
        "        return boxes,crops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYbI9BrRyDHm"
      },
      "source": [
        "Word Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpVWtFB6x_Uf"
      },
      "outputs": [],
      "source": [
        "def line_segmentation(image):\n",
        "    result_line = line.ocr(image,rec=False,cls=False)\n",
        "    return result_line\n",
        "\n",
        "def word_segmentation(image):\n",
        "    result_word = word.ocr(image,rec=False,cls=False)\n",
        "    return result_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeKMWbb0DG5x"
      },
      "outputs": [],
      "source": [
        "ONNX_PATH = \"/content/drive/MyDrive/reconstruction/bnocr.onnx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-t29QF9yYbf"
      },
      "outputs": [],
      "source": [
        "global det\n",
        "global rec\n",
        "\n",
        "det=Detector()\n",
        "rec=BanglaOCR(ONNX_PATH)\n",
        "\n",
        "\n",
        "def word_horizontal_dilation(boxes, image):\n",
        "    crops = []\n",
        "    length = len(boxes)\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "\n",
        "        if i+1 < length:\n",
        "            [[x_min, y_min], [x_max, y_min], [x_max, y_max], [x_min, y_max]] = boxes[i]\n",
        "            [[x_min1, y_min1], [x_max1, y_min1], [x_max1, y_max1], [x_min1, y_max1]] = boxes[i+1]\n",
        "\n",
        "            right_gap = x_min1 - x_max\n",
        "            if right_gap > 0:\n",
        "                x_max += right_gap // 2\n",
        "                x_min1 -= right_gap //2\n",
        "\n",
        "                boxes[i][1][0], boxes[i][2][0] = x_max, x_max\n",
        "                boxes[i+1][0][0], boxes[i+1][3][0] = x_min1, x_min1\n",
        "\n",
        "                crop = image[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
        "                h,w,d = crop.shape\n",
        "                if h!=0 and w!=0:\n",
        "                    crops.append(crop)\n",
        "\n",
        "    crop = image[int(boxes[-1][0][1]):int(boxes[-1][2][1]), int(boxes[-1][0][0]):int(boxes[-1][1][0])]\n",
        "    h, w, d = crop.shape\n",
        "    if h!=0 and w!=0:\n",
        "        crops.append(crop)\n",
        "\n",
        "    return crops\n",
        "\n",
        "\n",
        "def crop_word_regions(image, result_word):\n",
        "    boxes,crops=det(image, result_word[0])\n",
        "    return crops, boxes\n",
        "\n",
        "\n",
        "def recognize_word(crops):\n",
        "    texts = rec(crops)\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-BMeGWZ51xa"
      },
      "outputs": [],
      "source": [
        "# word wrap\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXbwUtkVyAaW"
      },
      "source": [
        "YOLOv8 Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smi-6Tp0XIfP"
      },
      "source": [
        "# Visualize Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abVj2flBUUzH"
      },
      "outputs": [],
      "source": [
        "def yolo(model_weight,image_path):\n",
        "    model = YOLO(model_weight)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # upscaling 2x\n",
        "    # height, width = image.shape[:2]\n",
        "    # image = cv2.resize(image, (2*width, 2*height))\n",
        "\n",
        "\n",
        "    # plt.imshow(image)\n",
        "    color_map = {\n",
        "        'text_box':   'red',\n",
        "        'paragraph':  'blue',\n",
        "        'image':   'green',\n",
        "        'table':  'yellow',\n",
        "    }\n",
        "\n",
        "    # layout_predicted = model(image)\n",
        "    res = model(image)\n",
        "    res_plotted = res[0].plot(conf=False, labels=False)\n",
        "\n",
        "    resized = cv2.resize(res_plotted, (500, 500))\n",
        "\n",
        "    cv2_imshow(resized)\n",
        "    return res\n",
        "\n",
        "\n",
        "def crop_all_text_box(image_path, res):\n",
        "    croped_imgs=[]\n",
        "    image = cv2.imread(image_path)\n",
        "    for i in range(len(res[0].boxes)):\n",
        "\n",
        "        x = int(res[0].boxes[i].xyxy[0][0])\n",
        "        y = int(res[0].boxes[i].xyxy[0][1])\n",
        "        width = int(res[0].boxes[i].xyxy[0][2] - res[0].boxes[i].xyxy[0][0])\n",
        "        height = int(res[0].boxes[i].xyxy[0][3] - res[0].boxes[i].xyxy[0][1])\n",
        "\n",
        "        crop_img = image[y:y+height, x:x+width]\n",
        "        croped_imgs.append(crop_img)\n",
        "        cv2_imshow(crop_img)\n",
        "    return croped_imgs\n",
        "\n",
        "\n",
        "def single_image_layout(model_weight,image_path,config_yml):\n",
        "    if len(config_yml)==0:\n",
        "        return yolo(model_weight,image_path)\n",
        "    # else:\n",
        "    #     return run_rcnn_model(model_weight,image_path,config_yml)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZKxe7FjUtGA"
      },
      "outputs": [],
      "source": [
        "image_path = '/content/drive/MyDrive/reconstruction/img_to_be_reconstructed/ekal1.jpg'\n",
        "config_yml = ''\n",
        "model_weight ='/content/drive/MyDrive/reconstruction/best.pt'\n",
        "\n",
        "\n",
        "result = single_image_layout(model_weight,image_path,config_yml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc7aWvmtXPJH"
      },
      "source": [
        "# HTML Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOavheXM-HjR"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from bs4.formatter import HTMLFormatter\n",
        "\n",
        "class HtmlGenerator:\n",
        "    def __init__(self, filename=\"default\"):\n",
        "        with open(\"/content/drive/MyDrive/reconstruction/templates/index.html\", 'r') as f:\n",
        "            index_template = f.read()\n",
        "\n",
        "        self.index_template = BeautifulSoup(index_template, \"html.parser\")\n",
        "        self.index_template_root_div = self.index_template.find(\"div\", {\"id\": \"root\"})\n",
        "        self.filename = filename\n",
        "\n",
        "    def read_html_template(self, template_name):\n",
        "        with open(f\"/content/drive/MyDrive/reconstruction/templates/{template_name}.html\", 'r') as f:\n",
        "            template = f.read()\n",
        "            soup_template = BeautifulSoup(template, \"html.parser\")\n",
        "            return soup_template\n",
        "\n",
        "\n",
        "    def get_styles(self, dict):\n",
        "        styles = f'top: {dict[\"top\"]}vh; left: {dict[\"left\"]}vw; height: {dict[\"elem_height\"]}vh; width: {dict[\"elem_width\"]}vw;'\n",
        "        return styles\n",
        "\n",
        "\n",
        "    def insert_paragraph(self, paragraph_info):\n",
        "        paragraph_template = self.read_html_template(\"paragraph\")\n",
        "\n",
        "        p_tag = paragraph_template.find('p')\n",
        "        text = paragraph_template.new_string(paragraph_info['text'])\n",
        "        p_tag.append(text)\n",
        "\n",
        "        paragraph_div = paragraph_template.find(\"div\")\n",
        "        paragraph_div[\"style\"] = self.get_styles(paragraph_info)\n",
        "\n",
        "        self.index_template_root_div.append(paragraph_template)\n",
        "\n",
        "\n",
        "    def insert_text_box(self, text_box_info):\n",
        "      text_box_template = self.read_html_template(\"text_box\")\n",
        "\n",
        "      p_tag = text_box_template.find('p')\n",
        "      text = text_box_template.new_string(text_box_info['text'])\n",
        "      p_tag.append(text)\n",
        "\n",
        "      text_box_div = text_box_template.find(\"div\")\n",
        "      text_box_div[\"style\"] =self.get_styles(text_box_info)\n",
        "\n",
        "      self.index_template_root_div.append(text_box_template)\n",
        "\n",
        "\n",
        "    def insert_image(self, img_info):\n",
        "\n",
        "        image_template = self.read_html_template(\"image\")\n",
        "\n",
        "        img_div = image_template.find(\"div\")\n",
        "        img_div[\"style\"] = self.get_styles(img_info)\n",
        "\n",
        "        img_tag = image_template.new_tag('img')\n",
        "        img_tag['src'] = img_info['img_src']\n",
        "\n",
        "        img_style = \"width: 100%; height: 100%; object-fit: fill;\"\n",
        "        img_tag['style'] = img_style\n",
        "\n",
        "        img_div.append(img_tag)\n",
        "\n",
        "        self.index_template_root_div.append(image_template)\n",
        "\n",
        "\n",
        "    def create_html_file(self):\n",
        "        with open(f\"/content/drive/MyDrive/reconstruction/html_output/{self.filename}.html\", \"w\") as f:\n",
        "            f.write(str(self.index_template.prettify(formatter=HTMLFormatter(indent=2))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m9j535i-NQn"
      },
      "outputs": [],
      "source": [
        "def generate_html(detected_elements_info, file_name):\n",
        "    file_name, extension = file_name.split(\".\")\n",
        "\n",
        "    gen = HtmlGenerator(file_name)\n",
        "\n",
        "    for element_info in detected_elements_info:\n",
        "\n",
        "        if element_info['class'] == 'paragraph':\n",
        "            gen.insert_paragraph(element_info)\n",
        "\n",
        "        elif element_info['class'] == 'text_box':\n",
        "            gen.insert_text_box(element_info)\n",
        "\n",
        "        elif element_info['class'] == 'image':\n",
        "            gen.insert_image(element_info)\n",
        "\n",
        "    gen.create_html_file()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJfxcP8Y1P97"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from shapely.geometry import box\n",
        "import time\n",
        "\n",
        "config_yml = ''\n",
        "model_weight ='/content/drive/MyDrive/reconstruction/best.pt'\n",
        "\n",
        "\n",
        "def get_normalized_coordinates(xyxy_tensor, height, width):\n",
        "    x_min = xyxy_tensor[0][0].item() / width\n",
        "    y_min = xyxy_tensor[0][1].item() / height\n",
        "    x_max = xyxy_tensor[0][2].item() / width\n",
        "    y_max = xyxy_tensor[0][3].item() / height\n",
        "\n",
        "    coordinates = [x_min, y_min, x_max, y_max]\n",
        "    return coordinates\n",
        "\n",
        "\n",
        "def get_original_coordinates(normalized_coordinates, image_width, image_height):\n",
        "    orig_coordinates = [None]*4\n",
        "\n",
        "    orig_coordinates[0] = math.floor(normalized_coordinates[0] * image_width)\n",
        "    orig_coordinates[1] = math.floor(normalized_coordinates[1] * image_height)\n",
        "    orig_coordinates[2] = math.ceil(normalized_coordinates[2] * image_width)\n",
        "    orig_coordinates[3] = math.ceil(normalized_coordinates[3] * image_height)\n",
        "\n",
        "    return orig_coordinates\n",
        "\n",
        "\n",
        "def get_coordinates_from_segmentation(result_word):\n",
        "    words_xyxy = []\n",
        "\n",
        "    for i in range(len(result_word[0])):\n",
        "        [[x_min, y_min], [x_max, y_min], [x_max, y_max], [x_min, y_max]] = result_word[0][i]\n",
        "        words_xyxy.append([math.floor(x_min), math.floor(y_min), math.ceil(x_max), math.ceil(y_max)])\n",
        "\n",
        "    return words_xyxy\n",
        "\n",
        "\n",
        "def line_vertical_dilation(line_coordinates):\n",
        "\n",
        "    for i in range(len(line_coordinates)):\n",
        "\n",
        "        if i+1 < len(line_coordinates):\n",
        "            [x_min, y_min, x_max, y_max] = line_coordinates[i]\n",
        "            [x_min1, y_min1, x_max1, y_max1] = line_coordinates[i+1]\n",
        "\n",
        "            bottom_gap = y_min1 - y_max\n",
        "            if bottom_gap > 0:\n",
        "                y_max += bottom_gap // 2\n",
        "                y_min1 -= bottom_gap //2\n",
        "\n",
        "                line_coordinates[i][3] = y_max\n",
        "                line_coordinates[i+1][1] = y_min1\n",
        "\n",
        "    return line_coordinates\n",
        "\n",
        "\n",
        "def top_bottom_padding(cropped_text_region):\n",
        "    h, w = cropped_text_region.shape[:2]\n",
        "    padded_height = int(h * 1.2)\n",
        "    padded_width = w\n",
        "\n",
        "    padded_image = np.ones((padded_height, padded_width, 3), dtype=np.uint8) * 255\n",
        "\n",
        "    top_padding = (h*2 - h) // 2\n",
        "    bottom_padding = top_padding + h\n",
        "\n",
        "    padded_image[top_padding:bottom_padding, :] = cropped_text_region\n",
        "\n",
        "    return padded_image\n",
        "\n",
        "\n",
        "def merge_image_arrays(element_wise_crop_list):\n",
        "    merged_array = []\n",
        "    for word_crops in element_wise_crop_list:\n",
        "        for crop in word_crops:\n",
        "            merged_array.append(crop)\n",
        "\n",
        "    return merged_array\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-pEMdUFA21DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LSOrFr7p21Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = {0: 'paragraph', 1: 'text_box', 2: 'image', 3: 'table'}\n",
        "\n",
        "def run_yolo_model(model_weight, image_path, file_name, img_src_save_directory):\n",
        "\n",
        "    file_name, extension = file_name.split('.')\n",
        "\n",
        "\n",
        "    model = YOLO(model_weight)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # upscaling 2x\n",
        "    # height, width = image.shape[:2]\n",
        "    # image = cv2.resize(image, (2*width, 2*height))\n",
        "\n",
        "    # plt.imshow(image)\n",
        "    color_map = {\n",
        "        'text_box':   'red',\n",
        "        'paragraph':  'blue',\n",
        "        'image':   'green',\n",
        "        'table':  'yellow',\n",
        "    }\n",
        "\n",
        "    # layout_predicted = model(image)\n",
        "    res = model(image)[0]\n",
        "\n",
        "    region_of_interests = []\n",
        "    num_of_words = []\n",
        "    word_crops = []\n",
        "\n",
        "    for i in range(len(res.boxes)):\n",
        "        info_dict = {\"class\": None,\n",
        "                    \"coordinates\": None,\n",
        "                    \"left\": None,\n",
        "                    \"top\": None,\n",
        "                    \"elem_height\": None,\n",
        "                    \"elem_width\": None,\n",
        "                    \"img_height\": None,\n",
        "                    \"img_width\": None,\n",
        "                    \"text\": '',\n",
        "                    \"single-line\": False,\n",
        "                    \"img_src\": None\n",
        "                    }\n",
        "\n",
        "        cls = res.boxes[i].cls.item()\n",
        "\n",
        "        img_height, img_width = res.boxes[i].orig_shape\n",
        "\n",
        "        normalized_coordinates = get_normalized_coordinates(res.boxes[i].xyxy, img_height, img_width)\n",
        "\n",
        "        if cls == 0:\n",
        "            info_dict['class'] = names[0]\n",
        "        elif cls == 1:\n",
        "            info_dict['class'] = names[1]\n",
        "        elif cls == 2:\n",
        "            info_dict['class'] = names[2]\n",
        "        elif cls == 3:\n",
        "            info_dict['class'] = names[3]\n",
        "\n",
        "        info_dict['coordinates'] = normalized_coordinates\n",
        "        info_dict['left'], info_dict['top'] = normalized_coordinates[0]*100, normalized_coordinates[1]*100\n",
        "        info_dict['img_height'], info_dict['img_width'] = img_height, img_width\n",
        "        info_dict['elem_width'] = (normalized_coordinates[2] - normalized_coordinates[0]) * 100\n",
        "        info_dict['elem_height'] = (normalized_coordinates[3] - normalized_coordinates[1]) * 100\n",
        "\n",
        "\n",
        "        if info_dict['class']==\"paragraph\" or info_dict['class']==\"text_box\":\n",
        "            x_min, y_min, x_max, y_max = get_original_coordinates(normalized_coordinates, info_dict[\"img_width\"], info_dict[\"img_height\"])\n",
        "\n",
        "\n",
        "            cropped_text_region = image[y_min:y_max, x_min:x_max]\n",
        "            # print(y_min,y_max, x_min,x_max)\n",
        "            # cv2_imshow(cropped_text_region)\n",
        "\n",
        "            '''uncomment to enable padding'''\n",
        "            # cropped_text_region = top_bottom_padding(cropped_text_region)\n",
        "\n",
        "            result_line = line_segmentation(cropped_text_region)\n",
        "            # print(\"result_line\", result_line)\n",
        "\n",
        "            line_coordinates = get_coordinates_from_segmentation(result_line)\n",
        "            # print(\"line_coordinates\", line_coordinates)\n",
        "\n",
        "            '''sort line coordinates based on y_min'''\n",
        "            sorted_line_coordinates = sorted(line_coordinates, key = lambda x: x[1])\n",
        "            # print(\"sorted_line_coordinates\", sorted_line_coordinates)\n",
        "\n",
        "            '''uncomment to enable vertical dilation for line segments'''\n",
        "            # sorted_line_coordinates = line_vertical_dilation(sorted_line_coordinates)\n",
        "            # print(\"sorted_line_coordinates after dilation\", sorted_line_coordinates)\n",
        "\n",
        "            text = []\n",
        "\n",
        "            total_word_count = 0\n",
        "            for i in range(len(sorted_line_coordinates)):\n",
        "                cropped_line_region = cropped_text_region[sorted_line_coordinates[i][1]-5:sorted_line_coordinates[i][3]+5,\n",
        "                                                          sorted_line_coordinates[i][0]:sorted_line_coordinates[i][2]]\n",
        "\n",
        "                if len(sorted_line_coordinates) == 1:\n",
        "                  info_dict['single-line'] = True\n",
        "                  info_dict['elem_height'] *= 1.5\n",
        "\n",
        "                # if len(cropped_line_region) != 0:\n",
        "                  # cv2_imshow(cropped_line_region)\n",
        "                # print(\"cropped_line_region\", cropped_line_region)\n",
        "\n",
        "                # font_size += (sorted_line_coordinates[i][3] - sorted_line_coordinates[i][1]) / info_dict['img_height']\n",
        "\n",
        "                if len(cropped_line_region) != 0:\n",
        "                    result_word = word_segmentation(cropped_line_region)\n",
        "                    # print(\"result_word\", result_word)\n",
        "\n",
        "                    # word_coordinates = get_coordinates_from_segmentation(result_word)\n",
        "                    # print(\"word_coordinates\", word_coordinates)\n",
        "\n",
        "                    # sort line coordinates based on x_min\n",
        "                    sorted_result_word = sorted(result_word[0], key = lambda x: x[0][0])\n",
        "                    # print(\"sorted_result_word\", sorted_result_word)\n",
        "\n",
        "                    if len(sorted_result_word) != 0:\n",
        "\n",
        "                        crops, boxes = crop_word_regions(cropped_line_region, [sorted_result_word])\n",
        "                        total_word_count += len(crops)\n",
        "                        word_crops.append(crops)\n",
        "                        # for elem in crops:\n",
        "                        #     word_crops.append(elem)\n",
        "\n",
        "            num_of_words.append(total_word_count)\n",
        "\n",
        "\n",
        "        elif info_dict['class']==\"image\":\n",
        "            x_min, y_min, x_max, y_max = get_original_coordinates(normalized_coordinates, info_dict[\"img_width\"], info_dict[\"img_height\"])\n",
        "            cropped_image_region = image[y_min:y_max, x_min:x_max]\n",
        "            src = f'{img_src_save_directory}\\\\{file_name}_{i}.{extension}'\n",
        "            info_dict['img_src'] = src\n",
        "            cv2.imwrite(f'/content/drive/MyDrive/reconstruction/img_src/{file_name}_{i}.{extension}', cropped_image_region)\n",
        "\n",
        "\n",
        "        region_of_interests.append(info_dict)\n",
        "\n",
        "    word_crops = merge_image_arrays(word_crops)\n",
        "\n",
        "    rec_time = time.time()\n",
        "    texts = recognize_word(word_crops)\n",
        "    print(\"Recognition time\", time.time() - rec_time)\n",
        "\n",
        "\n",
        "\n",
        "    text_count = -1\n",
        "    start = 0\n",
        "    for elem in region_of_interests:\n",
        "        if elem['class'] == 'paragraph' or elem['class'] == 'text_box':\n",
        "            text_count += 1\n",
        "            end = start + num_of_words[text_count]\n",
        "\n",
        "            if end > len(texts):\n",
        "                end = len(texts)\n",
        "            text = ' '.join(texts[start:end])\n",
        "            elem['text'] = text\n",
        "            start = end\n",
        "\n",
        "\n",
        "    # handling overlapping text regions\n",
        "    discard_elements = []\n",
        "    for i, element in enumerate(region_of_interests):\n",
        "        bb1 = box(element['coordinates'][0], element['coordinates'][1], element['coordinates'][2], element['coordinates'][3])\n",
        "\n",
        "        for j, other_element in enumerate(region_of_interests):\n",
        "            if j > i:\n",
        "                bb2 = box(other_element['coordinates'][0], other_element['coordinates'][1], other_element['coordinates'][2], other_element['coordinates'][3])\n",
        "                intersection = bb1.intersection(bb2).area\n",
        "\n",
        "                iou = intersection / bb2.area\n",
        "                # print(i, iou)\n",
        "                if iou > 0.5:\n",
        "                    if j not in discard_elements: discard_elements.append(j)\n",
        "\n",
        "                # if bb1.area < bb2.area:\n",
        "                #     iou = intersection / bb1.area\n",
        "                #     if iou > 0.5:\n",
        "                #         if i not in discard_elements: discard_elements.append(i)\n",
        "                # else:\n",
        "                #     iou = intersection / bb2.area\n",
        "                #     if iou > 0.5:\n",
        "                #         if j not in discard_elements: discard_elements.append(j)\n",
        "\n",
        "    # print(\"discard\", discard_elements)\n",
        "\n",
        "    items_deleted = 0\n",
        "    for index in discard_elements:\n",
        "        del region_of_interests[index - items_deleted]\n",
        "        items_deleted += 1\n",
        "\n",
        "    return region_of_interests"
      ],
      "metadata": {
        "id": "FLOOSDVj1zbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqugOMaA_51U"
      },
      "source": [
        "Generate html file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOcYFLB7BW1G"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def reconstruct(directory, img_src_save_dir):\n",
        "    directory = '/content/drive/MyDrive/reconstruction/img_to_be_reconstructed'  # Replace with your test directory path\n",
        "    img_src_save_dir = 'D:\\\\BADLAD\\\\html_output\\\\Final Output\\\\img' # Replace with your image source directory path\n",
        "\n",
        "    # Iterate over all files in the directory\n",
        "    for file_name in os.listdir(directory):\n",
        "        if os.path.isfile(os.path.join(directory, file_name)):\n",
        "\n",
        "            file_path = directory + \"/\" + file_name\n",
        "\n",
        "            print(\"----------------------------------------------------------------------------\")\n",
        "            print(\"File name:\", file_name)\n",
        "\n",
        "            start_time = time.time()\n",
        "            roi = run_yolo_model(model_weight, file_path, file_name, img_src_save_dir)\n",
        "            print(\"Execution Time for Layout Prediction and Text Recognition:\", round(time.time() - start_time, 2), \"seconds\")\n",
        "\n",
        "            start_time = time.time()\n",
        "            generate_html(roi, file_name)\n",
        "            print(\"Execution Time for Reconstruction:\", round(time.time() - start_time, 2), \"seconds\")\n",
        "            print(\"----------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(threshold=np.inf)"
      ],
      "metadata": {
        "id": "I1i9Ve9m20iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdsE41ddoYCb"
      },
      "outputs": [],
      "source": [
        "test_image_directory = '/content/drive/MyDrive/reconstruction/img_to_be_reconstructed'\n",
        "img_src_save_dir = 'D:\\\\BADLAD\\\\html_output\\\\Final Output\\\\img'\n",
        "reconstruct(test_image_directory, img_src_save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1O-HJBdXOowJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}