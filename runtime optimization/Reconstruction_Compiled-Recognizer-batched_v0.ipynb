{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WEzLk7f7QDga",
    "outputId": "9bf0bf59-8d27-4414-e551-0ea6d6f6ab77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Fri_Dec_17_18:28:54_Pacific_Standard_Time_2021\n",
      "Cuda compilation tools, release 11.6, V11.6.55\n",
      "Build cuda_11.6.r11.6/compiler.30794723_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116 True 1.24.4\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "import torch, torchvision,numpy\n",
    "print(torch.__version__, torch.cuda.is_available(), numpy.__version__) #1.13.1 True 1.23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9UPlcjg5Rf4J"
   },
   "outputs": [],
   "source": [
    "import layoutparser as lp\n",
    "import cv2\n",
    "import yaml\n",
    "import io\n",
    "import os\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "qCq8twBJ4tTY",
    "outputId": "0ea360e2-7bf7-442e-ebf4-0a74cf14a17e"
   },
   "outputs": [],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "import os\n",
    "import time\n",
    "# from google.colab.patches import cv2_imshow\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Kh3630vS3p9w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023/07/30 12:15:22] ppocr DEBUG: Namespace(alpha=1.0, benchmark=False, beta=1.0, cls_batch_num=6, cls_image_shape='3, 48, 192', cls_model_dir='C:\\\\Users\\\\Admin/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_thresh=0.9, cpu_threads=10, crop_res_save_dir='./output', det=True, det_algorithm='DB', det_box_type='quad', det_db_box_thresh=0.6, det_db_score_mode='fast', det_db_thresh=0.3, det_db_unclip_ratio=1.5, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_east_score_thresh=0.8, det_limit_side_len=960, det_limit_type='max', det_model_dir='C:\\\\Users\\\\Admin/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, det_pse_thresh=0, det_sast_nms_thresh=0.2, det_sast_score_thresh=0.5, draw_img_save_dir='./inference_results', drop_score=0.5, e2e_algorithm='PGNet', e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_limit_side_len=768, e2e_limit_type='max', e2e_model_dir=None, e2e_pgnet_mode='fast', e2e_pgnet_score_thresh=0.5, e2e_pgnet_valid_set='totaltext', enable_mkldnn=False, fourier_degree=5, gpu_mem=500, help='==SUPPRESS==', image_dir=None, image_orientation=False, ir_optim=True, kie_algorithm='LayoutXLM', label_list=['0', '180'], lang='en', layout=True, layout_dict_path=None, layout_model_dir=None, layout_nms_threshold=0.5, layout_score_threshold=0.5, max_batch_size=10, max_text_length=25, merge_no_span_structure=True, min_subgraph_size=15, mode='structure', ocr=True, ocr_order_method=None, ocr_version='PP-OCRv3', output='./output', page_num=0, precision='fp32', process_id=0, re_model_dir=None, rec=True, rec_algorithm='SVTR_LCNet', rec_batch_num=6, rec_char_dict_path='C:\\\\Users\\\\Admin\\\\AppData\\\\Roaming\\\\Python\\\\Python38\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\en_dict.txt', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_model_dir='C:\\\\Users\\\\Admin/.paddleocr/whl\\\\rec\\\\en\\\\en_PP-OCRv3_rec_infer', recovery=False, save_crop_res=False, save_log_path='./log_output/', scales=[8, 16, 32], ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ser_model_dir=None, show_log=True, sr_batch_num=1, sr_image_shape='3, 32, 128', sr_model_dir=None, structure_version='PP-StructureV2', table=True, table_algorithm='TableAttn', table_char_dict_path=None, table_max_len=488, table_model_dir=None, total_process_num=1, type='ocr', use_angle_cls=False, use_dilation=True, use_gpu=False, use_mp=False, use_npu=False, use_onnx=False, use_pdf2docx_api=False, use_pdserving=False, use_space_char=True, use_tensorrt=False, use_visual_backbone=True, use_xpu=False, vis_font_path='./doc/fonts/simfang.ttf', warmup=False)\n",
      "[2023/07/30 12:15:23] ppocr DEBUG: Namespace(alpha=1.0, benchmark=False, beta=1.0, cls_batch_num=6, cls_image_shape='3, 48, 192', cls_model_dir='C:\\\\Users\\\\Admin/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_thresh=0.9, cpu_threads=10, crop_res_save_dir='./output', det=True, det_algorithm='DB', det_box_type='quad', det_db_box_thresh=0.6, det_db_score_mode='fast', det_db_thresh=0.3, det_db_unclip_ratio=1.5, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_east_score_thresh=0.8, det_limit_side_len=960, det_limit_type='max', det_model_dir='C:\\\\Users\\\\Admin/.paddleocr/whl\\\\det\\\\ml\\\\Multilingual_PP-OCRv3_det_infer', det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, det_pse_thresh=0, det_sast_nms_thresh=0.2, det_sast_score_thresh=0.5, draw_img_save_dir='./inference_results', drop_score=0.5, e2e_algorithm='PGNet', e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_limit_side_len=768, e2e_limit_type='max', e2e_model_dir=None, e2e_pgnet_mode='fast', e2e_pgnet_score_thresh=0.5, e2e_pgnet_valid_set='totaltext', enable_mkldnn=False, fourier_degree=5, gpu_mem=500, help='==SUPPRESS==', image_dir=None, image_orientation=False, ir_optim=True, kie_algorithm='LayoutXLM', label_list=['0', '180'], lang='ar', layout=True, layout_dict_path=None, layout_model_dir=None, layout_nms_threshold=0.5, layout_score_threshold=0.5, max_batch_size=10, max_text_length=25, merge_no_span_structure=True, min_subgraph_size=15, mode='structure', ocr=True, ocr_order_method=None, ocr_version='PP-OCRv3', output='./output', page_num=0, precision='fp32', process_id=0, re_model_dir=None, rec=True, rec_algorithm='SVTR_LCNet', rec_batch_num=6, rec_char_dict_path='C:\\\\Users\\\\Admin\\\\AppData\\\\Roaming\\\\Python\\\\Python38\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\dict\\\\arabic_dict.txt', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_model_dir='C:\\\\Users\\\\Admin/.paddleocr/whl\\\\rec\\\\arabic\\\\arabic_PP-OCRv3_rec_infer', recovery=False, save_crop_res=False, save_log_path='./log_output/', scales=[8, 16, 32], ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ser_model_dir=None, show_log=True, sr_batch_num=1, sr_image_shape='3, 32, 128', sr_model_dir=None, structure_version='PP-StructureV2', table=True, table_algorithm='TableAttn', table_char_dict_path=None, table_max_len=488, table_model_dir=None, total_process_num=1, type='ocr', use_angle_cls=False, use_dilation=True, use_gpu=False, use_mp=False, use_npu=False, use_onnx=False, use_pdf2docx_api=False, use_pdserving=False, use_space_char=True, use_tensorrt=False, use_visual_backbone=True, use_xpu=False, vis_font_path='./doc/fonts/simfang.ttf', warmup=False)\n"
     ]
    }
   ],
   "source": [
    "line=PaddleOCR(use_angle_cls=False, lang='en',use_gpu=True, use_dilation=True) #, det_model_dir=r'C:\\\\Users\\\\Admin/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_slim_infer')\n",
    "word=PaddleOCR(use_angle_cls=False, lang='ar',use_gpu=True, use_dilation=True) #, det_model_dir=r'C:\\\\Users\\\\Admin/.paddleocr/whl\\\\det\\\\ml\\\\Multilingual_PP-OCRv3_det_slim_infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JM_UMiM0EctA"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#-------------------------\n",
    "# imports\n",
    "#-------------------------\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import cv2\n",
    "from bnunicodenormalizer import Normalizer\n",
    "NORM=Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2lMrQfRC3-bH"
   },
   "outputs": [],
   "source": [
    "class BanglaOCR(object):\n",
    "    def __init__(self,\n",
    "                model_weights,\n",
    "                providers=['CUDAExecutionProvider'],\n",
    "                img_height=32,\n",
    "                img_width=256,\n",
    "                pos_max=40):\n",
    "        self.img_height=img_height\n",
    "        self.img_width =img_width\n",
    "        self.pos_max   =pos_max\n",
    "        self.model     =ort.InferenceSession(model_weights, providers=providers)\n",
    "        self.vocab     =[\"blank\",\"!\",\"\\\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\"=\",\">\",\"?\",\"।\",\n",
    "                        \"ঁ\",\"ং\",\"ঃ\",\"অ\",\"আ\",\"ই\",\"ঈ\",\"উ\",\"ঊ\",\"ঋ\",\"এ\",\"ঐ\",\"ও\",\"ঔ\",\n",
    "                        \"ক\",\"খ\",\"গ\",\"ঘ\",\"ঙ\",\"চ\",\"ছ\",\"জ\",\"ঝ\",\"ঞ\",\"ট\",\"ঠ\",\"ড\",\"ঢ\",\"ণ\",\"ত\",\"থ\",\"দ\",\"ধ\",\"ন\",\n",
    "                        \"প\",\"ফ\",\"ব\",\"ভ\",\"ম\",\"য\",\"র\",\"ল\",\"শ\",\"ষ\",\"স\",\"হ\",\n",
    "                        \"া\",\"ি\",\"ী\",\"ু\",\"ূ\",\"ৃ\",\"ে\",\"ৈ\",\"ো\",\"ৌ\",\"্\",\n",
    "                        \"ৎ\",\"ড়\",\"ঢ়\",\"য়\",\"০\",\"১\",\"২\",\"৩\",\"৪\",\"৫\",\"৬\",\"৭\",\"৮\",\"৯\",\"‍\",\"sep\",\"pad\"]\n",
    "\n",
    "    def process_batch(self,crops):\n",
    "        batch_img=[]\n",
    "        batch_pos=[]\n",
    "        for img in crops:\n",
    "            # correct padding\n",
    "            img,_=correctPadding(img,(self.img_height,self.img_width))\n",
    "            # normalize\n",
    "            img=img/255.0\n",
    "            # extend batch\n",
    "            img=np.expand_dims(img,axis=0)\n",
    "            batch_img.append(img)\n",
    "            # pos\n",
    "            pos=np.array([[i for i in range(self.pos_max)]])\n",
    "            batch_pos.append(pos)\n",
    "        # stack\n",
    "        img=np.vstack(batch_img)\n",
    "        img=img.astype(np.float32)\n",
    "        pos=np.vstack(batch_pos)\n",
    "        pos=pos.astype(np.float32)\n",
    "        # batch inp\n",
    "        return {\"image\":img,\"pos\":pos}\n",
    "\n",
    "    def __call__(self,crops,batch_size=32):\n",
    "        # adjust batch_size\n",
    "        if len(crops)<batch_size:\n",
    "            batch_size=len(crops)\n",
    "        texts=[]\n",
    "        for idx in range(0,len(crops),batch_size):\n",
    "            batch=crops[idx:idx+batch_size]\n",
    "            inp=self.process_batch(batch)\n",
    "            preds=self.model.run(None,inp)[0]\n",
    "            preds =np.argmax(preds,axis=-1)\n",
    "            # decoding\n",
    "            for pred in preds:\n",
    "                label=\"\"\n",
    "                for c in pred[1:]:\n",
    "                    if c!=self.vocab.index(\"sep\"):\n",
    "                        label+=self.vocab[c]\n",
    "                    else:\n",
    "                        break\n",
    "                texts.append(label)\n",
    "        texts=[NORM(text)[\"normalized\"] for text in texts]\n",
    "        texts=[text for text in texts if text is not None]\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "N_3FEhk5Hi8o"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# ---------------------------------------------------------\n",
    "# imports\n",
    "# ---------------------------------------------------------\n",
    "import cv2\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-_GIMovzH00n"
   },
   "outputs": [],
   "source": [
    "def padWordImage(img,pad_loc,pad_dim,pad_val):\n",
    "    '''\n",
    "        pads an image with white value\n",
    "        args:\n",
    "            img     :       the image to pad\n",
    "            pad_loc :       (lr/tb) lr: left-right pad , tb=top_bottom pad\n",
    "            pad_dim :       the dimension to pad upto\n",
    "            pad_val :       the value to pad\n",
    "    '''\n",
    "\n",
    "    if pad_loc==\"lr\":\n",
    "        # shape\n",
    "        h,w,d=img.shape\n",
    "        # pad widths\n",
    "        pad_width =pad_dim-w\n",
    "        # pads\n",
    "        pad =np.ones((h,pad_width,3))*pad_val\n",
    "        # pad\n",
    "        img =np.concatenate([img,pad],axis=1)\n",
    "    else:\n",
    "        # shape\n",
    "        h,w,d=img.shape\n",
    "        # pad heights\n",
    "        if h>= pad_dim:\n",
    "            return img\n",
    "        else:\n",
    "            pad_height =pad_dim-h\n",
    "            # pads\n",
    "            pad =np.ones((pad_height,w,3))*pad_val\n",
    "            # pad\n",
    "            img =np.concatenate([img,pad],axis=0)\n",
    "    return img.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SWWskKZiHrlB"
   },
   "outputs": [],
   "source": [
    "def correctPadding(img,dim,pvalue=255):\n",
    "    '''\n",
    "        corrects an image padding\n",
    "        args:\n",
    "            img     :       numpy array of single channel image\n",
    "            dim     :       tuple of desired img_height,img_width\n",
    "            pvalue  :       the value to pad\n",
    "        returns:\n",
    "            correctly padded image\n",
    "\n",
    "    '''\n",
    "    img_height,img_width=dim\n",
    "    mask=0\n",
    "    # check for pad\n",
    "    h,w,d=img.shape\n",
    "\n",
    "    w_new=int(img_height* w/h)\n",
    "    img=cv2.resize(img,(w_new,img_height))\n",
    "    h,w,d=img.shape\n",
    "\n",
    "    if w > img_width:\n",
    "        # for larger width\n",
    "        h_new= int(img_width* h/w)\n",
    "        img=cv2.resize(img,(img_width,h_new))\n",
    "        # pad\n",
    "        img=padWordImage(img,\n",
    "                     pad_loc=\"tb\",\n",
    "                     pad_dim=img_height,\n",
    "                     pad_val=pvalue)\n",
    "        mask=img_width\n",
    "\n",
    "    elif w < img_width:\n",
    "        # pad\n",
    "        img=padWordImage(img,\n",
    "                    pad_loc=\"lr\",\n",
    "                    pad_dim=img_width,\n",
    "                    pad_val=pvalue)\n",
    "        mask=w\n",
    "\n",
    "    # error avoid\n",
    "    img=cv2.resize(img,(img_width,img_height))\n",
    "\n",
    "    return img,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "c1JVg6LVDqY5"
   },
   "outputs": [],
   "source": [
    "class Detector(object):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "            initializes a dbnet detector model\n",
    "        '''\n",
    "        self.call_rec=\"paddle\"\n",
    "\n",
    "    def sorted_boxes(self,dt_boxes,dist=10):\n",
    "        \"\"\"\n",
    "        Sort text boxes in order from top to bottom, left to right\n",
    "        args:\n",
    "            dt_boxes(array):detected text boxes with shape [4, 2]\n",
    "        return:\n",
    "            sorted boxes(array) with shape [4, 2]\n",
    "        \"\"\"\n",
    "        num_boxes = dt_boxes.shape[0]\n",
    "        sorted_boxes = sorted(dt_boxes, key=lambda x: (x[0][1], x[0][0]))\n",
    "        _boxes = list(sorted_boxes)\n",
    "\n",
    "        for i in range(num_boxes - 1):\n",
    "            if abs(_boxes[i + 1][0][1] - _boxes[i][0][1]) < dist and (_boxes[i + 1][0][0] < _boxes[i][0][0]):\n",
    "                tmp = _boxes[i]\n",
    "                _boxes[i] = _boxes[i + 1]\n",
    "                _boxes[i + 1] = tmp\n",
    "        return _boxes\n",
    "\n",
    "    def get_rotate_crop_image(self,img, points):\n",
    "        # Use Green's theory to judge clockwise or counterclockwise\n",
    "        # author: biyanhua\n",
    "        d = 0.0\n",
    "        for index in range(-1, 3):\n",
    "            d += -0.5 * (points[index + 1][1] + points[index][1]) * (\n",
    "                        points[index + 1][0] - points[index][0])\n",
    "        if d < 0: # counterclockwise\n",
    "            tmp = np.array(points)\n",
    "            points[1], points[3] = tmp[3], tmp[1]\n",
    "\n",
    "        img_crop_width = int(\n",
    "            max(\n",
    "                np.linalg.norm(points[0] - points[1]),\n",
    "                np.linalg.norm(points[2] - points[3])))\n",
    "        img_crop_height = int(\n",
    "            max(\n",
    "                np.linalg.norm(points[0] - points[3]),\n",
    "                np.linalg.norm(points[1] - points[2])))\n",
    "        pts_std = np.float32([[0, 0], [img_crop_width, 0],\n",
    "                            [img_crop_width, img_crop_height],\n",
    "                            [0, img_crop_height]])\n",
    "        M = cv2.getPerspectiveTransform(points, pts_std)\n",
    "        dst_img = cv2.warpPerspective(\n",
    "            img,\n",
    "            M, (img_crop_width, img_crop_height),\n",
    "            borderMode=cv2.BORDER_REPLICATE,\n",
    "            flags=cv2.INTER_CUBIC)\n",
    "        dst_img_height, dst_img_width = dst_img.shape[0:2]\n",
    "        if dst_img_height * 1.0 / dst_img_width >= 1.5:\n",
    "            dst_img = np.rot90(dst_img)\n",
    "        return dst_img\n",
    "\n",
    "\n",
    "    def __call__(self,img,result):\n",
    "        '''\n",
    "            extract locations and crops\n",
    "        '''\n",
    "        boxes= np.array(result, dtype=np.float32)\n",
    "\n",
    "        # boxes=self.sorted_boxes(boxes) # This existed in the original code\n",
    "\n",
    "        crops=[]\n",
    "        for bno in range(len(boxes)):\n",
    "            tmp_box = copy.deepcopy(boxes[bno])\n",
    "            img_crop = self.get_rotate_crop_image(img,tmp_box)\n",
    "            crops.append(img_crop)\n",
    "        #mask=create_mask(img,boxes)\n",
    "        #return mask,boxes,crops\n",
    "        return boxes,crops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYbI9BrRyDHm"
   },
   "source": [
    "Word Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IpVWtFB6x_Uf"
   },
   "outputs": [],
   "source": [
    "def line_segmentation(image):\n",
    "    result_line = line.ocr(image,rec=False,cls=False)\n",
    "    return result_line\n",
    "\n",
    "def word_segmentation(image):\n",
    "    result_word = word.ocr(image,rec=False,cls=False)\n",
    "    return result_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oeKMWbb0DG5x"
   },
   "outputs": [],
   "source": [
    "# def quantize_onnx_model(onnx_model_path, quantized_model_path):\n",
    "#     from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "#     import onnx\n",
    "#     onnx_opt_model = onnx.load(onnx_model_path)\n",
    "#     quantize_dynamic(onnx_model_path,\n",
    "#                      quantized_model_path,\n",
    "#                      weight_type=QuantType.QUInt8)\n",
    "\n",
    "#     print(f\"quantized model saved to:{quantized_model_path}\")\n",
    "\n",
    "# quantize_onnx_model(\"bnocr.onnx\", \"quantized_bnocr.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONNX_PATH = \"bnocr.onnx\"\n",
    "# ONNX_PATH = \"./quantized_bnocr.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_-t29QF9yYbf"
   },
   "outputs": [],
   "source": [
    "global det\n",
    "global rec\n",
    "\n",
    "det=Detector()\n",
    "rec=BanglaOCR(ONNX_PATH)\n",
    "\n",
    "\n",
    "def word_horizontal_dilation(boxes, image):\n",
    "    crops = []\n",
    "    length = len(boxes)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "\n",
    "        if i+1 < length:\n",
    "            [[x_min, y_min], [x_max, y_min], [x_max, y_max], [x_min, y_max]] = boxes[i]\n",
    "            [[x_min1, y_min1], [x_max1, y_min1], [x_max1, y_max1], [x_min1, y_max1]] = boxes[i+1]\n",
    "\n",
    "            right_gap = x_min1 - x_max\n",
    "            if right_gap > 0:\n",
    "                x_max += right_gap // 2\n",
    "                x_min1 -= right_gap //2\n",
    "\n",
    "                boxes[i][1][0], boxes[i][2][0] = x_max, x_max\n",
    "                boxes[i+1][0][0], boxes[i+1][3][0] = x_min1, x_min1\n",
    "\n",
    "                crop = image[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
    "                h,w,d = crop.shape\n",
    "                if h!=0 and w!=0:\n",
    "                    crops.append(crop)\n",
    "\n",
    "    crop = image[int(boxes[-1][0][1]):int(boxes[-1][2][1]), int(boxes[-1][0][0]):int(boxes[-1][1][0])]\n",
    "    h, w, d = crop.shape\n",
    "    if h!=0 and w!=0:\n",
    "        crops.append(crop)\n",
    "\n",
    "    return crops\n",
    "\n",
    "\n",
    "def crop_word_regions(image, result_word):\n",
    "    boxes,crops=det(image, result_word[0])\n",
    "    return crops, boxes\n",
    "\n",
    "\n",
    "def recognize_word(crops):\n",
    "    texts = rec(crops)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXbwUtkVyAaW"
   },
   "source": [
    "YOLOv8 Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smi-6Tp0XIfP"
   },
   "source": [
    "# Visualize Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "abVj2flBUUzH"
   },
   "outputs": [],
   "source": [
    "def yolo(model_weight,image_path):\n",
    "    model = YOLO(model_weight)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # upscaling 2x\n",
    "    # height, width = image.shape[:2]\n",
    "    # image = cv2.resize(image, (2*width, 2*height))\n",
    "\n",
    "\n",
    "    # plt.imshow(image)\n",
    "    color_map = {\n",
    "        'text_box':   'red',\n",
    "        'paragraph':  'blue',\n",
    "        'image':   'green',\n",
    "        'table':  'yellow',\n",
    "    }\n",
    "\n",
    "    # layout_predicted = model(image)\n",
    "    res = model(image)\n",
    "    res_plotted = res[0].plot(conf=False, labels=False)\n",
    "\n",
    "    resized = cv2.resize(res_plotted, (500, 500))\n",
    "\n",
    "    cv2.imshow('Resized Image', resized)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    return res\n",
    "\n",
    "\n",
    "def crop_all_text_box(image_path, res):\n",
    "    croped_imgs=[]\n",
    "    image = cv2.imread(image_path)\n",
    "    for i in range(len(res[0].boxes)):\n",
    "\n",
    "        x = int(res[0].boxes[i].xyxy[0][0])\n",
    "        y = int(res[0].boxes[i].xyxy[0][1])\n",
    "        width = int(res[0].boxes[i].xyxy[0][2] - res[0].boxes[i].xyxy[0][0])\n",
    "        height = int(res[0].boxes[i].xyxy[0][3] - res[0].boxes[i].xyxy[0][1])\n",
    "\n",
    "        crop_img = image[y:y+height, x:x+width]\n",
    "        croped_imgs.append(crop_img)\n",
    "        cv2.imshow('Document Layout Detected', crop_img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    return croped_imgs\n",
    "\n",
    "\n",
    "def single_image_layout(model_weight,image_path,config_yml):\n",
    "    if len(config_yml)==0:\n",
    "        return yolo(model_weight,image_path)\n",
    "    # else:\n",
    "    #     return run_rcnn_model(model_weight,image_path,config_yml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PZKxe7FjUtGA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 10 paragraphs, 1 text_box, 294.3ms\n",
      "Speed: 513.5ms preprocess, 294.3ms inference, 713.6ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "image_path = 'test_images\\\\two_col.png'\n",
    "config_yml = ''\n",
    "model_weight ='best.pt'\n",
    "\n",
    "\n",
    "result = single_image_layout(model_weight,image_path,config_yml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc7aWvmtXPJH"
   },
   "source": [
    "# HTML Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SOavheXM-HjR"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.formatter import HTMLFormatter\n",
    "\n",
    "class HtmlGenerator:\n",
    "    def __init__(self, filename=\"default\"):\n",
    "        with open(\"templates/index.html\", 'r') as f:\n",
    "            index_template = f.read()\n",
    "\n",
    "        self.index_template = BeautifulSoup(index_template, \"html.parser\")\n",
    "        self.index_template_root_div = self.index_template.find(\"div\", {\"id\": \"root\"})\n",
    "        self.filename = filename\n",
    "\n",
    "    def read_html_template(self, template_name):\n",
    "        with open(f\"templates/{template_name}.html\", 'r') as f:\n",
    "            template = f.read()\n",
    "            soup_template = BeautifulSoup(template, \"html.parser\")\n",
    "            return soup_template\n",
    "\n",
    "\n",
    "    def get_styles(self, dict):\n",
    "        styles = f'top: {dict[\"top\"]}vh; left: {dict[\"left\"]}vw; height: {dict[\"elem_height\"]}vh; width: {dict[\"elem_width\"]}vw;'\n",
    "        return styles\n",
    "\n",
    "\n",
    "    def insert_paragraph(self, paragraph_info):\n",
    "        paragraph_template = self.read_html_template(\"paragraph\")\n",
    "\n",
    "        p_tag = paragraph_template.find('p')\n",
    "        text = paragraph_template.new_string(paragraph_info['text'])\n",
    "        p_tag.append(text)\n",
    "\n",
    "        paragraph_div = paragraph_template.find(\"div\")\n",
    "        paragraph_div[\"style\"] = self.get_styles(paragraph_info)\n",
    "\n",
    "        self.index_template_root_div.append(paragraph_template)\n",
    "\n",
    "\n",
    "    def insert_text_box(self, text_box_info):\n",
    "      text_box_template = self.read_html_template(\"text_box\")\n",
    "\n",
    "      p_tag = text_box_template.find('p')\n",
    "      text = text_box_template.new_string(text_box_info['text'])\n",
    "      p_tag.append(text)\n",
    "\n",
    "      text_box_div = text_box_template.find(\"div\")\n",
    "      text_box_div[\"style\"] =self.get_styles(text_box_info)\n",
    "\n",
    "      self.index_template_root_div.append(text_box_template)\n",
    "\n",
    "\n",
    "    def insert_image(self, img_info):\n",
    "\n",
    "        image_template = self.read_html_template(\"image\")\n",
    "\n",
    "        img_div = image_template.find(\"div\")\n",
    "        img_div[\"style\"] = self.get_styles(img_info)\n",
    "\n",
    "        img_tag = image_template.new_tag('img')\n",
    "        img_tag['src'] = img_info['img_src']\n",
    "\n",
    "        img_style = \"width: 100%; height: 100%; object-fit: fill;\"\n",
    "        img_tag['style'] = img_style\n",
    "\n",
    "        img_div.append(img_tag)\n",
    "\n",
    "        self.index_template_root_div.append(image_template)\n",
    "\n",
    "\n",
    "    def create_html_file(self):\n",
    "        with open(f\"html/{self.filename}.html\", \"w\") as f:\n",
    "            f.write(str(self.index_template.prettify(formatter=HTMLFormatter(indent=2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "2m9j535i-NQn"
   },
   "outputs": [],
   "source": [
    "def generate_html(detected_elements_info, file_name):\n",
    "    file_name, extension = file_name.split(\".\")\n",
    "\n",
    "    gen = HtmlGenerator(file_name)\n",
    "\n",
    "    for element_info in detected_elements_info:\n",
    "\n",
    "        if element_info['class'] == 'paragraph':\n",
    "            gen.insert_paragraph(element_info)\n",
    "\n",
    "        elif element_info['class'] == 'text_box':\n",
    "            gen.insert_text_box(element_info)\n",
    "\n",
    "        elif element_info['class'] == 'image':\n",
    "            gen.insert_image(element_info)\n",
    "\n",
    "    gen.create_html_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "hJfxcP8Y1P97"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from shapely.geometry import box\n",
    "import time\n",
    "\n",
    "config_yml = ''\n",
    "model_weight ='best.pt'\n",
    "\n",
    "\n",
    "def get_normalized_coordinates(xyxy_tensor, height, width):\n",
    "    x_min = xyxy_tensor[0][0].item() / width\n",
    "    y_min = xyxy_tensor[0][1].item() / height\n",
    "    x_max = xyxy_tensor[0][2].item() / width\n",
    "    y_max = xyxy_tensor[0][3].item() / height\n",
    "\n",
    "    coordinates = [x_min, y_min, x_max, y_max]\n",
    "    return coordinates\n",
    "\n",
    "\n",
    "def get_original_coordinates(normalized_coordinates, image_width, image_height):\n",
    "    orig_coordinates = [None]*4\n",
    "\n",
    "    orig_coordinates[0] = math.floor(normalized_coordinates[0] * image_width)\n",
    "    orig_coordinates[1] = math.floor(normalized_coordinates[1] * image_height)\n",
    "    orig_coordinates[2] = math.ceil(normalized_coordinates[2] * image_width)\n",
    "    orig_coordinates[3] = math.ceil(normalized_coordinates[3] * image_height)\n",
    "\n",
    "    return orig_coordinates\n",
    "\n",
    "\n",
    "def get_coordinates_from_segmentation(result_word):\n",
    "    words_xyxy = []\n",
    "\n",
    "    for i in range(len(result_word[0])):\n",
    "        [[x_min, y_min], [x_max, y_min], [x_max, y_max], [x_min, y_max]] = result_word[0][i]\n",
    "        words_xyxy.append([math.floor(x_min), math.floor(y_min), math.ceil(x_max), math.ceil(y_max)])\n",
    "\n",
    "    return words_xyxy\n",
    "\n",
    "\n",
    "def line_vertical_dilation(line_coordinates):\n",
    "\n",
    "    for i in range(len(line_coordinates)):\n",
    "\n",
    "        if i+1 < len(line_coordinates):\n",
    "            [x_min, y_min, x_max, y_max] = line_coordinates[i]\n",
    "            [x_min1, y_min1, x_max1, y_max1] = line_coordinates[i+1]\n",
    "\n",
    "            bottom_gap = y_min1 - y_max\n",
    "            if bottom_gap > 0:\n",
    "                y_max += bottom_gap // 2\n",
    "                y_min1 -= bottom_gap //2\n",
    "\n",
    "                line_coordinates[i][3] = y_max\n",
    "                line_coordinates[i+1][1] = y_min1\n",
    "\n",
    "    return line_coordinates\n",
    "\n",
    "\n",
    "def top_bottom_padding(cropped_text_region):\n",
    "    h, w = cropped_text_region.shape[:2]\n",
    "    padded_height = int(h * 1.2)\n",
    "    padded_width = w\n",
    "\n",
    "    padded_image = np.ones((padded_height, padded_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "    top_padding = (padded_height - h) // 2\n",
    "    bottom_padding = top_padding + h\n",
    "\n",
    "    padded_image[top_padding:bottom_padding, :] = cropped_text_region\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "\n",
    "def merge_image_arrays(element_wise_crop_list):\n",
    "    merged_array = []\n",
    "    for word_crops in element_wise_crop_list:\n",
    "        for crop in word_crops:\n",
    "            merged_array.append(crop)\n",
    "\n",
    "    return merged_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "FLOOSDVj1zbh"
   },
   "outputs": [],
   "source": [
    "names = {0: 'paragraph', 1: 'text_box', 2: 'image', 3: 'table'}\n",
    "\n",
    "def run_yolo_model(model_weight, image_path, file_name, img_src_save_directory):\n",
    "\n",
    "    file_name, extension = file_name.split('.')\n",
    "\n",
    "\n",
    "    model = YOLO(model_weight)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # upscaling 2x\n",
    "    # height, width = image.shape[:2]\n",
    "    # image = cv2.resize(image, (2*width, 2*height))\n",
    "\n",
    "    # plt.imshow(image)\n",
    "    color_map = {\n",
    "        'text_box':   'red',\n",
    "        'paragraph':  'blue',\n",
    "        'image':   'green',\n",
    "        'table':  'yellow',\n",
    "    }\n",
    "\n",
    "    # layout_predicted = model(image)\n",
    "    res = model(image)[0]\n",
    "\n",
    "    region_of_interests = []\n",
    "    num_of_words = []\n",
    "    word_crops = []\n",
    "    all_result_lines = []\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(len(res.boxes)):\n",
    "        info_dict = {\"class\": None,\n",
    "                    \"coordinates\": None,\n",
    "                    \"left\": None,\n",
    "                    \"top\": None,\n",
    "                    \"elem_height\": None,\n",
    "                    \"elem_width\": None,\n",
    "                    \"img_height\": None,\n",
    "                    \"img_width\": None,\n",
    "                    \"text\": '',\n",
    "                    \"single-line\": False,\n",
    "                    \"img_src\": None\n",
    "                    }\n",
    "\n",
    "        cls = res.boxes[i].cls.item()\n",
    "\n",
    "        img_height, img_width = res.boxes[i].orig_shape\n",
    "\n",
    "        normalized_coordinates = get_normalized_coordinates(res.boxes[i].xyxy, img_height, img_width)\n",
    "\n",
    "        if cls == 0:\n",
    "            info_dict['class'] = names[0]\n",
    "        elif cls == 1:\n",
    "            info_dict['class'] = names[1]\n",
    "        elif cls == 2:\n",
    "            info_dict['class'] = names[2]\n",
    "        elif cls == 3:\n",
    "            info_dict['class'] = names[3]\n",
    "\n",
    "        info_dict['coordinates'] = normalized_coordinates\n",
    "        info_dict['left'], info_dict['top'] = normalized_coordinates[0]*100, normalized_coordinates[1]*100\n",
    "        info_dict['img_height'], info_dict['img_width'] = img_height, img_width\n",
    "        info_dict['elem_width'] = (normalized_coordinates[2] - normalized_coordinates[0]) * 100\n",
    "        info_dict['elem_height'] = (normalized_coordinates[3] - normalized_coordinates[1]) * 100\n",
    "\n",
    "\n",
    "        if info_dict['class']==\"paragraph\" or info_dict['class']==\"text_box\":\n",
    "            x_min, y_min, x_max, y_max = get_original_coordinates(normalized_coordinates, info_dict[\"img_width\"], info_dict[\"img_height\"])\n",
    "\n",
    "\n",
    "            cropped_text_region = image[y_min:y_max, x_min:x_max]\n",
    "            # print(y_min,y_max, x_min,x_max)\n",
    "            # cv2_imshow(cropped_text_region)\n",
    "\n",
    "            '''uncomment to enable padding'''\n",
    "            cropped_text_region = top_bottom_padding(cropped_text_region)\n",
    "\n",
    "            result_line = line_segmentation(cropped_text_region)\n",
    "            # print(\"result_line\", result_line)\n",
    "\n",
    "            line_coordinates = get_coordinates_from_segmentation(result_line)\n",
    "            # print(\"line_coordinates\", line_coordinates)\n",
    "\n",
    "            '''sort line coordinates based on y_min'''\n",
    "            sorted_line_coordinates = sorted(line_coordinates, key = lambda x: x[1])\n",
    "            # print(\"sorted_line_coordinates\", sorted_line_coordinates)\n",
    "\n",
    "            '''uncomment to enable vertical dilation for line segments'''\n",
    "            sorted_line_coordinates = line_vertical_dilation(sorted_line_coordinates)\n",
    "            # print(\"sorted_line_coordinates after dilation\", sorted_line_coordinates)\n",
    "\n",
    "            text = []\n",
    "\n",
    "            total_word_count = 0\n",
    "            \n",
    "            for i in range(len(sorted_line_coordinates)):\n",
    "                cropped_line_region = cropped_text_region[sorted_line_coordinates[i][1]-5:sorted_line_coordinates[i][3]+5,\n",
    "                                                          sorted_line_coordinates[i][0]:sorted_line_coordinates[i][2]]\n",
    "\n",
    "                if len(sorted_line_coordinates) == 1:\n",
    "                  info_dict['single-line'] = True\n",
    "                  info_dict['elem_height'] *= 1.5\n",
    "\n",
    "                if len(cropped_line_region) != 0:\n",
    "                  # cv2_imshow(cropped_line_region)\n",
    "                    cv2.imwrite(f\"D:\\\\BADLAD\\\\__RECONSTRUCTION\\\\visualization\\\\cropped_line_regions\\\\{counter}.jpg\", cropped_line_region)\n",
    "                    counter += 1\n",
    "                # print(\"cropped_line_region\", cropped_line_region)\n",
    "\n",
    "\n",
    "                if len(cropped_line_region) != 0:\n",
    "                    result_word = word_segmentation(cropped_line_region)\n",
    "                    # print(\"result_word\", result_word)\n",
    "\n",
    "                    # word_coordinates = get_coordinates_from_segmentation(result_word)\n",
    "                    # print(\"word_coordinates\", word_coordinates)\n",
    "\n",
    "                    # sort words based on x_min\n",
    "                    sorted_result_word = sorted(result_word[0], key = lambda x: x[0][0])\n",
    "                    # print(\"sorted_result_word\", sorted_result_word)\n",
    "\n",
    "                    if len(sorted_result_word) != 0:\n",
    "\n",
    "                        crops, boxes = crop_word_regions(cropped_line_region, [sorted_result_word])\n",
    "                        total_word_count += len(crops)\n",
    "                        word_crops.append(crops)\n",
    "                        \n",
    "\n",
    "            num_of_words.append(total_word_count)\n",
    "\n",
    "\n",
    "        elif info_dict['class']==\"image\":\n",
    "            x_min, y_min, x_max, y_max = get_original_coordinates(normalized_coordinates, info_dict[\"img_width\"], info_dict[\"img_height\"])\n",
    "            cropped_image_region = image[y_min:y_max, x_min:x_max]\n",
    "            src = f'{img_src_save_directory}/{file_name}_{i}.{extension}'\n",
    "            info_dict['img_src'] = src\n",
    "            cv2.imwrite(f'html/img/{file_name}_{i}.{extension}', cropped_image_region)\n",
    "\n",
    "\n",
    "        region_of_interests.append(info_dict)\n",
    "\n",
    "    word_crops = merge_image_arrays(word_crops)\n",
    "\n",
    "    for idx, crop in enumerate(word_crops):\n",
    "        cv2.imwrite(f\"visualization\\\\cropped_word_regions\\\\{idx}.jpg\", crop)\n",
    "\n",
    "\n",
    "    rec_time = time.time()\n",
    "    texts = recognize_word(word_crops)\n",
    "    # print(texts)\n",
    "    print(\"Recognition time\", time.time() - rec_time)\n",
    "\n",
    "\n",
    "\n",
    "    text_count = -1\n",
    "    start = 0\n",
    "    for elem in region_of_interests:\n",
    "        if elem['class'] == 'paragraph' or elem['class'] == 'text_box':\n",
    "            text_count += 1\n",
    "            end = start + num_of_words[text_count]\n",
    "\n",
    "            if end > len(texts):\n",
    "                end = len(texts)\n",
    "            text = ' '.join(texts[start:end])\n",
    "            elem['text'] = text\n",
    "            start = end\n",
    "\n",
    "\n",
    "    # handling overlapping text regions\n",
    "    discard_elements = []\n",
    "    for i, element in enumerate(region_of_interests):\n",
    "        bb1 = box(element['coordinates'][0], element['coordinates'][1], element['coordinates'][2], element['coordinates'][3])\n",
    "\n",
    "        for j, other_element in enumerate(region_of_interests):\n",
    "            if j > i:\n",
    "                bb2 = box(other_element['coordinates'][0], other_element['coordinates'][1], other_element['coordinates'][2], other_element['coordinates'][3])\n",
    "                intersection = bb1.intersection(bb2).area\n",
    "\n",
    "                iou = intersection / bb2.area\n",
    "                # print(i, iou)\n",
    "                if iou > 0.5:\n",
    "                    if j not in discard_elements: discard_elements.append(j)\n",
    "\n",
    "                # if bb1.area < bb2.area:\n",
    "                #     iou = intersection / bb1.area\n",
    "                #     if iou > 0.5:\n",
    "                #         if i not in discard_elements: discard_elements.append(i)\n",
    "                # else:\n",
    "                #     iou = intersection / bb2.area\n",
    "                #     if iou > 0.5:\n",
    "                #         if j not in discard_elements: discard_elements.append(j)\n",
    "\n",
    "    # print(\"discard\", discard_elements)\n",
    "\n",
    "    items_deleted = 0\n",
    "    for index in discard_elements:\n",
    "        del region_of_interests[index - items_deleted]\n",
    "        items_deleted += 1\n",
    "\n",
    "    return region_of_interests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqugOMaA_51U"
   },
   "source": [
    "Generate html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "KOcYFLB7BW1G"
   },
   "outputs": [],
   "source": [
    "def reconstruct(directory, img_src_save_dir):\n",
    "\n",
    "    # Iterate over all files in the directory\n",
    "    for file_name in os.listdir(directory):\n",
    "        if os.path.isfile(os.path.join(directory, file_name)):\n",
    "\n",
    "            file_path = directory + \"/\" + file_name\n",
    "\n",
    "            print(\"----------------------------------------------------------------------------\")\n",
    "            print(\"File name:\", file_name)\n",
    "\n",
    "            start_time = time.time()\n",
    "            roi = run_yolo_model(model_weight, file_path, file_name, img_src_save_dir)\n",
    "            print(\"Execution Time for Layout Prediction and Text Recognition:\", round(time.time() - start_time, 2), \"seconds\")\n",
    "\n",
    "            start_time = time.time()\n",
    "            generate_html(roi, file_name)\n",
    "            print(\"Execution Time for Reconstruction:\", round(time.time() - start_time, 2), \"seconds\")\n",
    "            print(\"----------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "File name: 000a84c1-12f1-4977-acaf-071b42281d93.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 4 paragraphs, 72 text_boxs, 4 images, 1 table, 42.0ms\n",
      "Speed: 2.0ms preprocess, 42.0ms inference, 9.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition time 3.446173667907715\n",
      "Execution Time for Layout Prediction and Text Recognition: 11.02 seconds\n",
      "Execution Time for Reconstruction: 1.52 seconds\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "File name: 000be499-dd8a-49b1-8cd9-a9a792e8fb38.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 35 paragraphs, 1 text_box, 1 image, 55.1ms\n",
      "Speed: 2.0ms preprocess, 55.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition time 4.88447642326355\n",
      "Execution Time for Layout Prediction and Text Recognition: 9.77 seconds\n",
      "Execution Time for Reconstruction: 0.02 seconds\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "File name: 00a3c9df-7931-4438-ae87-8878e69a439e.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 3 paragraphs, 1 text_box, 37.0ms\n",
      "Speed: 2.0ms preprocess, 37.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition time 2.1569626331329346\n",
      "Execution Time for Layout Prediction and Text Recognition: 4.6 seconds\n",
      "Execution Time for Reconstruction: 0.0 seconds\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "File name: dc (1).png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x416 2 paragraphs, 31.0ms\n",
      "Speed: 3.0ms preprocess, 31.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition time 1.0779812335968018\n",
      "Execution Time for Layout Prediction and Text Recognition: 4.53 seconds\n",
      "Execution Time for Reconstruction: 0.0 seconds\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "File name: ekal.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x448 30 paragraphs, 68 text_boxs, 18 images, 34.0ms\n",
      "Speed: 2.0ms preprocess, 34.0ms inference, 10.0ms postprocess per image at shape (1, 3, 640, 448)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition time 10.709424018859863\n",
      "Execution Time for Layout Prediction and Text Recognition: 25.24 seconds\n",
      "Execution Time for Reconstruction: 0.04 seconds\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "File name: ekal1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x448 49 paragraphs, 42 text_boxs, 7 images, 39.5ms\n",
      "Speed: 2.0ms preprocess, 39.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition time 17.00023126602173\n",
      "Execution Time for Layout Prediction and Text Recognition: 38.32 seconds\n",
      "Execution Time for Reconstruction: 0.03 seconds\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "File name: front_page.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x448 4 text_boxs, 38.0ms\n",
      "Speed: 3.0ms preprocess, 38.0ms inference, 50.0ms postprocess per image at shape (1, 3, 640, 448)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition time 0.23121070861816406\n",
      "Execution Time for Layout Prediction and Text Recognition: 0.94 seconds\n",
      "Execution Time for Reconstruction: 0.0 seconds\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "File name: f_page.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x448 4 text_boxs, 34.0ms\n",
      "Speed: 1.0ms preprocess, 34.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 448)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition time 0.10609602928161621\n",
      "Execution Time for Layout Prediction and Text Recognition: 0.74 seconds\n",
      "Execution Time for Reconstruction: 0.0 seconds\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "File name: newspaper.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 88 paragraphs, 62 text_boxs, 5 images, 36.0ms\n",
      "Speed: 2.0ms preprocess, 36.0ms inference, 12.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition time 7.269614934921265\n",
      "Execution Time for Layout Prediction and Text Recognition: 18.84 seconds\n",
      "Execution Time for Reconstruction: 0.06 seconds\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "File name: paper.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 24 paragraphs, 3 text_boxs, 1 image, 36.0ms\n",
      "Speed: 3.0ms preprocess, 36.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition time 3.7724356651306152\n",
      "Execution Time for Layout Prediction and Text Recognition: 7.61 seconds\n",
      "Execution Time for Reconstruction: 0.01 seconds\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "File name: photo 1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 19 paragraphs, 1 text_box, 1 image, 36.0ms\n",
      "Speed: 2.0ms preprocess, 36.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition time 4.460968971252441\n",
      "Execution Time for Layout Prediction and Text Recognition: 8.37 seconds\n",
      "Execution Time for Reconstruction: 0.01 seconds\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "File name: photo.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 6 paragraphs, 4 text_boxs, 1 image, 37.0ms\n",
      "Speed: 2.0ms preprocess, 37.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition time 2.991281509399414\n",
      "Execution Time for Layout Prediction and Text Recognition: 5.61 seconds\n",
      "Execution Time for Reconstruction: 0.01 seconds\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "File name: two_col (1).png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 8 paragraphs, 5 text_boxs, 36.0ms\n",
      "Speed: 1.0ms preprocess, 36.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition time 4.930060863494873\n",
      "Execution Time for Layout Prediction and Text Recognition: 8.72 seconds\n",
      "Execution Time for Reconstruction: 0.01 seconds\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "File name: two_col (2).png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 17 paragraphs, 3 text_boxs, 36.0ms\n",
      "Speed: 5.0ms preprocess, 36.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition time 7.40465784072876\n",
      "Execution Time for Layout Prediction and Text Recognition: 12.4 seconds\n",
      "Execution Time for Reconstruction: 0.01 seconds\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "File name: two_col.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 10 paragraphs, 1 text_box, 36.0ms\n",
      "Speed: 4.0ms preprocess, 36.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition time 7.246561288833618\n",
      "Execution Time for Layout Prediction and Text Recognition: 11.27 seconds\n",
      "Execution Time for Reconstruction: 0.01 seconds\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_image_directory = 'img_to_be_reconstructed'\n",
    "img_src_save_dir = 'D:/BADLAD/__RECONSTRUCTION/html/img'\n",
    "reconstruct(test_image_directory, img_src_save_dir)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
