{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY0WMSQJPaX-",
        "outputId": "4d4fdd32-6d4b-4442-b89f-846acbd5466c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni_5n8ka23u7",
        "outputId": "bf0a78af-1125-4d97-dc38-960aa8b8da1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEzLk7f7QDga",
        "outputId": "ee664fc3-4669-4a6a-d3e4-0ed5c370fd14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "2.0.1+cu118 True 1.22.4\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "import torch, torchvision,numpy\n",
        "print(torch.__version__, torch.cuda.is_available(), numpy.__version__) #1.13.1 True 1.23.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mA5mG7br4rBZ",
        "outputId": "c955f2bd-8f61-44f3-c3ac-317f8b8d6de1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m569.6/569.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.7/375.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.9/445.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m813.9/813.9 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.4/148.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.3/231.3 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# !sudo apt-get install -q gcc-7\n",
        "\n",
        "!python -m pip install -q paddlepaddle-gpu==2.3.0.post111 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html\n",
        "# !pip install paddleocr==2.5.0.3\n",
        "\n",
        "!pip install -q paddlepaddle-gpu -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
        "\n",
        "!pip install -q \"paddleocr>=2.0.1\" # Recommend to use version 2.0.1+\n",
        "# !pip install -q dist/paddleocr-2.0.1-py3-none-any.whl # x.x.x is the version of paddleocr\n",
        "\n",
        "!pip install -q paddleocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qCq8twBJ4tTY",
        "outputId": "36db53e7-063b-47d5-bd25-aa3306ef7a2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ce956a0b62ac>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Yolo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlayoutparser\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'layoutparser'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from tqdm import tqdm_notebook as tqdm # progress bar\n",
        "from datetime import datetime\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "#from pycocotools.coco import COCO\n",
        "import os, json, cv2, random\n",
        "import skimage.io as io\n",
        "import copy\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "\n",
        "# Albumenatations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "#from pycocotools.coco import COCO\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# glob\n",
        "from glob import glob\n",
        "\n",
        "# numba\n",
        "# import numba\n",
        "# from numba import jit\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') #Ignore \"future\" warnings and Data-Frame-Slicing warnings.\n",
        "\n",
        "# Yolo\n",
        "import layoutparser as lp\n",
        "import cv2\n",
        "import yaml\n",
        "import io\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "from google.colab.patches import cv2_imshow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OE6iwnGGQkDA"
      },
      "outputs": [],
      "source": [
        "!pip install -q ultralytics\n",
        "!pip install -q -U layoutparser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9UPlcjg5Rf4J"
      },
      "outputs": [],
      "source": [
        "import layoutparser as lp\n",
        "import cv2\n",
        "import yaml\n",
        "import io\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Kh3630vS3p9w"
      },
      "outputs": [],
      "source": [
        "line=PaddleOCR(use_angle_cls=False, lang='en',use_gpu=True)\n",
        "word=PaddleOCR(use_angle_cls=False, lang='ar',use_gpu=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MW4aLopgF2Ch"
      },
      "outputs": [],
      "source": [
        "!pip install -q bnunicodenormalizer\n",
        "!pip install -q onnxruntime-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JM_UMiM0EctA"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "#-------------------------\n",
        "# imports\n",
        "#-------------------------\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import cv2\n",
        "from bnunicodenormalizer import Normalizer\n",
        "NORM=Normalizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2lMrQfRC3-bH"
      },
      "outputs": [],
      "source": [
        "class BanglaOCR(object):\n",
        "    def __init__(self,\n",
        "                model_weights,\n",
        "                providers=['CUDAExecutionProvider'],\n",
        "                img_height=32,\n",
        "                img_width=256,\n",
        "                pos_max=40):\n",
        "        self.img_height=img_height\n",
        "        self.img_width =img_width\n",
        "        self.pos_max   =pos_max\n",
        "        self.model     =ort.InferenceSession(model_weights, providers=providers)\n",
        "        self.vocab     =[\"blank\",\"!\",\"\\\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\"=\",\">\",\"?\",\"।\",\n",
        "                        \"ঁ\",\"ং\",\"ঃ\",\"অ\",\"আ\",\"ই\",\"ঈ\",\"উ\",\"ঊ\",\"ঋ\",\"এ\",\"ঐ\",\"ও\",\"ঔ\",\n",
        "                        \"ক\",\"খ\",\"গ\",\"ঘ\",\"ঙ\",\"চ\",\"ছ\",\"জ\",\"ঝ\",\"ঞ\",\"ট\",\"ঠ\",\"ড\",\"ঢ\",\"ণ\",\"ত\",\"থ\",\"দ\",\"ধ\",\"ন\",\n",
        "                        \"প\",\"ফ\",\"ব\",\"ভ\",\"ম\",\"য\",\"র\",\"ল\",\"শ\",\"ষ\",\"স\",\"হ\",\n",
        "                        \"া\",\"ি\",\"ী\",\"ু\",\"ূ\",\"ৃ\",\"ে\",\"ৈ\",\"ো\",\"ৌ\",\"্\",\n",
        "                        \"ৎ\",\"ড়\",\"ঢ়\",\"য়\",\"০\",\"১\",\"২\",\"৩\",\"৪\",\"৫\",\"৬\",\"৭\",\"৮\",\"৯\",\"‍\",\"sep\",\"pad\"]\n",
        "\n",
        "    def process_batch(self,crops):\n",
        "        batch_img=[]\n",
        "        batch_pos=[]\n",
        "        for img in crops:\n",
        "            # correct padding\n",
        "            img,_=correctPadding(img,(self.img_height,self.img_width))\n",
        "            # normalize\n",
        "            img=img/255.0\n",
        "            # extend batch\n",
        "            img=np.expand_dims(img,axis=0)\n",
        "            batch_img.append(img)\n",
        "            # pos\n",
        "            pos=np.array([[i for i in range(self.pos_max)]])\n",
        "            batch_pos.append(pos)\n",
        "        # stack\n",
        "        img=np.vstack(batch_img)\n",
        "        img=img.astype(np.float32)\n",
        "        pos=np.vstack(batch_pos)\n",
        "        pos=pos.astype(np.float32)\n",
        "        # batch inp\n",
        "        return {\"image\":img,\"pos\":pos}\n",
        "\n",
        "    def __call__(self,crops,batch_size=32):\n",
        "        # adjust batch_size\n",
        "        if len(crops)<batch_size:\n",
        "            batch_size=len(crops)\n",
        "        texts=[]\n",
        "        for idx in range(0,len(crops),batch_size):\n",
        "            batch=crops[idx:idx+batch_size]\n",
        "            inp=self.process_batch(batch)\n",
        "            preds=self.model.run(None,inp)[0]\n",
        "            preds =np.argmax(preds,axis=-1)\n",
        "            # decoding\n",
        "            for pred in preds:\n",
        "                label=\"\"\n",
        "                for c in pred[1:]:\n",
        "                    if c!=self.vocab.index(\"sep\"):\n",
        "                        label+=self.vocab[c]\n",
        "                    else:\n",
        "                        break\n",
        "                texts.append(label)\n",
        "        texts=[NORM(text)[\"normalized\"] for text in texts]\n",
        "        texts=[text for text in texts if text is not None]\n",
        "        return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N_3FEhk5Hi8o"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "# ---------------------------------------------------------\n",
        "# imports\n",
        "# ---------------------------------------------------------\n",
        "import cv2\n",
        "import numpy as np\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-_GIMovzH00n"
      },
      "outputs": [],
      "source": [
        "def padWordImage(img,pad_loc,pad_dim,pad_val):\n",
        "    '''\n",
        "        pads an image with white value\n",
        "        args:\n",
        "            img     :       the image to pad\n",
        "            pad_loc :       (lr/tb) lr: left-right pad , tb=top_bottom pad\n",
        "            pad_dim :       the dimension to pad upto\n",
        "            pad_val :       the value to pad\n",
        "    '''\n",
        "\n",
        "    if pad_loc==\"lr\":\n",
        "        # shape\n",
        "        h,w,d=img.shape\n",
        "        # pad widths\n",
        "        pad_width =pad_dim-w\n",
        "        # pads\n",
        "        pad =np.ones((h,pad_width,3))*pad_val\n",
        "        # pad\n",
        "        img =np.concatenate([img,pad],axis=1)\n",
        "    else:\n",
        "        # shape\n",
        "        h,w,d=img.shape\n",
        "        # pad heights\n",
        "        if h>= pad_dim:\n",
        "            return img\n",
        "        else:\n",
        "            pad_height =pad_dim-h\n",
        "            # pads\n",
        "            pad =np.ones((pad_height,w,3))*pad_val\n",
        "            # pad\n",
        "            img =np.concatenate([img,pad],axis=0)\n",
        "    return img.astype(\"uint8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SWWskKZiHrlB"
      },
      "outputs": [],
      "source": [
        "def correctPadding(img,dim,pvalue=255):\n",
        "    '''\n",
        "        corrects an image padding\n",
        "        args:\n",
        "            img     :       numpy array of single channel image\n",
        "            dim     :       tuple of desired img_height,img_width\n",
        "            pvalue  :       the value to pad\n",
        "        returns:\n",
        "            correctly padded image\n",
        "\n",
        "    '''\n",
        "    img_height,img_width=dim\n",
        "    mask=0\n",
        "    # check for pad\n",
        "    h,w,d=img.shape\n",
        "\n",
        "    w_new=int(img_height* w/h)\n",
        "    img=cv2.resize(img,(w_new,img_height))\n",
        "    h,w,d=img.shape\n",
        "\n",
        "    if w > img_width:\n",
        "        # for larger width\n",
        "        h_new= int(img_width* h/w)\n",
        "        img=cv2.resize(img,(img_width,h_new))\n",
        "        # pad\n",
        "        img=padWordImage(img,\n",
        "                     pad_loc=\"tb\",\n",
        "                     pad_dim=img_height,\n",
        "                     pad_val=pvalue)\n",
        "        mask=img_width\n",
        "\n",
        "    elif w < img_width:\n",
        "        # pad\n",
        "        img=padWordImage(img,\n",
        "                    pad_loc=\"lr\",\n",
        "                    pad_dim=img_width,\n",
        "                    pad_val=pvalue)\n",
        "        mask=w\n",
        "\n",
        "    # error avoid\n",
        "    img=cv2.resize(img,(img_width,img_height))\n",
        "\n",
        "    return img,mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c1JVg6LVDqY5"
      },
      "outputs": [],
      "source": [
        "class Detector(object):\n",
        "    def __init__(self):\n",
        "        '''\n",
        "            initializes a dbnet detector model\n",
        "        '''\n",
        "        self.call_rec=\"paddle\"\n",
        "\n",
        "    def sorted_boxes(self,dt_boxes,dist=10):\n",
        "        \"\"\"\n",
        "        Sort text boxes in order from top to bottom, left to right\n",
        "        args:\n",
        "            dt_boxes(array):detected text boxes with shape [4, 2]\n",
        "        return:\n",
        "            sorted boxes(array) with shape [4, 2]\n",
        "        \"\"\"\n",
        "        num_boxes = dt_boxes.shape[0]\n",
        "        sorted_boxes = sorted(dt_boxes, key=lambda x: (x[0][1], x[0][0]))\n",
        "        _boxes = list(sorted_boxes)\n",
        "\n",
        "        for i in range(num_boxes - 1):\n",
        "            if abs(_boxes[i + 1][0][1] - _boxes[i][0][1]) < dist and (_boxes[i + 1][0][0] < _boxes[i][0][0]):\n",
        "                tmp = _boxes[i]\n",
        "                _boxes[i] = _boxes[i + 1]\n",
        "                _boxes[i + 1] = tmp\n",
        "        return _boxes\n",
        "\n",
        "    def get_rotate_crop_image(self,img, points):\n",
        "        # Use Green's theory to judge clockwise or counterclockwise\n",
        "        # author: biyanhua\n",
        "        d = 0.0\n",
        "        for index in range(-1, 3):\n",
        "            d += -0.5 * (points[index + 1][1] + points[index][1]) * (\n",
        "                        points[index + 1][0] - points[index][0])\n",
        "        if d < 0: # counterclockwise\n",
        "            tmp = np.array(points)\n",
        "            points[1], points[3] = tmp[3], tmp[1]\n",
        "\n",
        "        img_crop_width = int(\n",
        "            max(\n",
        "                np.linalg.norm(points[0] - points[1]),\n",
        "                np.linalg.norm(points[2] - points[3])))\n",
        "        img_crop_height = int(\n",
        "            max(\n",
        "                np.linalg.norm(points[0] - points[3]),\n",
        "                np.linalg.norm(points[1] - points[2])))\n",
        "        pts_std = np.float32([[0, 0], [img_crop_width, 0],\n",
        "                            [img_crop_width, img_crop_height],\n",
        "                            [0, img_crop_height]])\n",
        "        M = cv2.getPerspectiveTransform(points, pts_std)\n",
        "        dst_img = cv2.warpPerspective(\n",
        "            img,\n",
        "            M, (img_crop_width, img_crop_height),\n",
        "            borderMode=cv2.BORDER_REPLICATE,\n",
        "            flags=cv2.INTER_CUBIC)\n",
        "        dst_img_height, dst_img_width = dst_img.shape[0:2]\n",
        "        if dst_img_height * 1.0 / dst_img_width >= 1.5:\n",
        "            dst_img = np.rot90(dst_img)\n",
        "        return dst_img\n",
        "\n",
        "\n",
        "    def __call__(self,img,result):\n",
        "        '''\n",
        "            extract locations and crops\n",
        "        '''\n",
        "        boxes= np.array(result, dtype=np.float32)\n",
        "\n",
        "        # print(\"unbox:\", boxes)\n",
        "\n",
        "        # boxes=self.sorted_boxes(boxes) # This existed in the original code\n",
        "\n",
        "        # print(\"box:\", boxes)\n",
        "\n",
        "        crops=[]\n",
        "        for bno in range(len(boxes)):\n",
        "            tmp_box = copy.deepcopy(boxes[bno])\n",
        "            img_crop = self.get_rotate_crop_image(img,tmp_box)\n",
        "            crops.append(img_crop)\n",
        "        #mask=create_mask(img,boxes)\n",
        "        #return mask,boxes,crops\n",
        "        return boxes,crops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYbI9BrRyDHm"
      },
      "source": [
        "Word Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IpVWtFB6x_Uf"
      },
      "outputs": [],
      "source": [
        "def line_segmentation(image):\n",
        "    result_line = line.ocr(image,rec=False,cls=False)\n",
        "    return result_line\n",
        "\n",
        "def word_segmentation(image):\n",
        "    result_word = word.ocr(image,rec=False,cls=False)\n",
        "    return result_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oeKMWbb0DG5x"
      },
      "outputs": [],
      "source": [
        "ONNX_PATH = '/content/drive/MyDrive/BANGLAOCR/weights/bnocr.onnx'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_-t29QF9yYbf"
      },
      "outputs": [],
      "source": [
        "det=Detector()\n",
        "rec=BanglaOCR(ONNX_PATH)\n",
        "\n",
        "\n",
        "def recognize_word(image, result_word):\n",
        "\n",
        "    # img_path = image_path\n",
        "    # img=cv2.imread(img_path)\n",
        "    # boxes,crops=det(img,data[\"word\"])\n",
        "    boxes,crops=det(image,result_word[0])\n",
        "    texts=rec(crops)\n",
        "    return texts, crops, boxes\n",
        "\n",
        "\n",
        "# texts, crops, boxes = recognize_word(img, result_word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i-BMeGWZ51xa"
      },
      "outputs": [],
      "source": [
        "# word wrap\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "abVj2flBUUzH"
      },
      "outputs": [],
      "source": [
        "def yolo(model_weight,image_path):\n",
        "    model = YOLO(model_weight)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # upscaling 2x\n",
        "    # height, width = image.shape[:2]\n",
        "    # image = cv2.resize(image, (2*width, 2*height))\n",
        "\n",
        "\n",
        "    # plt.imshow(image)\n",
        "    color_map = {\n",
        "        'text_box':   'red',\n",
        "        'paragraph':  'blue',\n",
        "        'image':   'green',\n",
        "        'table':  'yellow',\n",
        "    }\n",
        "\n",
        "    # layout_predicted = model(image)\n",
        "    res = model(image)\n",
        "    res_plotted = res[0].plot()\n",
        "\n",
        "    resized = cv2.resize(res_plotted, (500, 500))\n",
        "\n",
        "    cv2_imshow(resized)\n",
        "    return res\n",
        "\n",
        "\n",
        "def crop_all_text_box(image_path, res):\n",
        "    croped_imgs=[]\n",
        "    print(\"hoise\")\n",
        "    image = cv2.imread(image_path)\n",
        "    print(\"2nd hoise\")\n",
        "    for i in range(len(res[0].boxes)):\n",
        "\n",
        "        x = int(res[0].boxes[i].xyxy[0][0])\n",
        "        y = int(res[0].boxes[i].xyxy[0][1])\n",
        "        width = int(res[0].boxes[i].xyxy[0][2] - res[0].boxes[i].xyxy[0][0])\n",
        "        height = int(res[0].boxes[i].xyxy[0][3] - res[0].boxes[i].xyxy[0][1])\n",
        "\n",
        "        crop_img = image[y:y+height, x:x+width]\n",
        "        croped_imgs.append(crop_img)\n",
        "        # cv2_imshow(crop_img)\n",
        "\n",
        "    return croped_imgs\n",
        "\n",
        "def crop_single_line(image, res):\n",
        "    # words[0][1]:words[0][3], words[0][0]:words[0][2]\n",
        "    x = int(res[0][0])\n",
        "    y = int(res[0][1])\n",
        "    width = int(res[0][2] - res[0][0])\n",
        "    height = int(res[0][3] - res[0][1])\n",
        "\n",
        "    crop_img = image[y:y+height, x:x+width]\n",
        "    # croped_imgs.append(crop_img)\n",
        "    # cv2_imshow(crop_img)\n",
        "\n",
        "    return crop_img\n",
        "\n",
        "# text_boxes = crop_all_text_box( image_path, res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NgZzNc_t-tdw"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "\n",
        "def process_batch(batch, image_path, results):\n",
        "    result = line_and_word_recognition(batch, image_path)\n",
        "    results.append(result)\n",
        "\n",
        "def line_segmentation_process_batch(batch, results):\n",
        "    for img in batch:\n",
        "        [result] = line_segmentation(img)\n",
        "        print(\"function run\")\n",
        "        results.append(result)\n",
        "\n",
        "def word_segmentation_process_batch(batch, results):\n",
        "    for img in batch:\n",
        "        result = word_segmentation(img)\n",
        "        results.append(result)\n",
        "\n",
        "def parallel_batch_processing(boxes, num_batches):\n",
        "    # multiprocessing.set_start_method('spawn',force=True)\n",
        "    # pool = multiprocessing.Pool(processes=num_batches)\n",
        "    batch_size = len(boxes) // num_batches\n",
        "    # Split the boxes into batches\n",
        "    batches = [boxes[i:i+batch_size] for i in range(0, len(boxes), batch_size)]\n",
        "\n",
        "    # Process the batches in parallel\n",
        "    # pool.map(line_and_word_recognition(result, image_path), batches)\n",
        "    # pool.map(line_and_word_recognition, [(batch, image_path) for batch in batches])\n",
        "    # pool.close()\n",
        "    # pool.join()\n",
        "    # Create and start a thread for each batch\n",
        "\n",
        "    # Create a list to store the batch outputs\n",
        "    region_of_interests = []\n",
        "\n",
        "    # Create and start a thread for each batch\n",
        "    threads = []\n",
        "    for batch in batches:\n",
        "        thread = threading.Thread(target=process_batch, args=(batch, image_path, region_of_interests))\n",
        "        thread.start()\n",
        "        threads.append(thread)\n",
        "\n",
        "    # Wait for all threads to finish\n",
        "    for thread in threads:\n",
        "        thread.join()\n",
        "\n",
        "    return region_of_interests\n",
        "\n",
        "\n",
        "def line_parallel_batch_processing(boxes, num_batches):\n",
        "    # multiprocessing.set_start_method('spawn',force=True)\n",
        "    # pool = multiprocessing.Pool(processes=num_batches)\n",
        "    batch_size = len(boxes) // num_batches\n",
        "    # Split the boxes into batches\n",
        "    batches = [boxes[i:i+batch_size] for i in range(0, len(boxes), batch_size)]\n",
        "\n",
        "    region_of_interests = []\n",
        "\n",
        "    # Create and start a thread for each batch\n",
        "    threads = []\n",
        "    for batch in batches:\n",
        "        print(batch)\n",
        "        print(\"thred run\")\n",
        "        thread = threading.Thread(target=line_segmentation_process_batch, args=(batch,region_of_interests))\n",
        "        thread.start()\n",
        "        threads.append(thread)\n",
        "\n",
        "    # Wait for all threads to finish\n",
        "    for thread in threads:\n",
        "        thread.join()\n",
        "\n",
        "    return region_of_interests\n",
        "\n",
        "\n",
        "def word_parallel_batch_processing(boxes, num_batches):\n",
        "    # multiprocessing.set_start_method('spawn',force=True)\n",
        "    # pool = multiprocessing.Pool(processes=num_batches)\n",
        "    batch_size = len(boxes) // num_batches\n",
        "    # Split the boxes into batches\n",
        "    batches = [boxes[i:i+batch_size] for i in range(0, len(boxes), batch_size)]\n",
        "\n",
        "    # Process the batches in parallel\n",
        "    # pool.map(line_and_word_recognition(result, image_path), batches)\n",
        "    # pool.map(line_and_word_recognition, [(batch, image_path) for batch in batches])\n",
        "    # pool.close()\n",
        "    # pool.join()\n",
        "    # Create and start a thread for each batch\n",
        "\n",
        "    # Create a list to store the batch outputs\n",
        "    region_of_interests = []\n",
        "\n",
        "    # Create and start a thread for each batch\n",
        "    threads = []\n",
        "    for batch in batches:\n",
        "        thread = threading.Thread(target=word_segmentation_process_batch, args=(batch, region_of_interests))\n",
        "        thread.start()\n",
        "        threads.append(thread)\n",
        "\n",
        "    # Wait for all threads to finish\n",
        "    for thread in threads:\n",
        "        thread.join()\n",
        "\n",
        "    return region_of_interests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gmwh83o2d3uG"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def get_coordinates_from_segmentation(result_word):\n",
        "    words_xyxy = []\n",
        "\n",
        "    for i in range(len(result_word[0])):\n",
        "        [[x_min, y_min], [x_max, y_min], [x_max, y_max], [x_min, y_max]] = result_word[0][i]\n",
        "        words_xyxy.append([math.floor(x_min), math.floor(y_min), math.ceil(x_max), math.ceil(y_max)])\n",
        "\n",
        "    return words_xyxy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PZKxe7FjUtGA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "image_path = '/content/drive/MyDrive/BANGLAOCR/Dataset/50_test_data/ebec2688-da21-4e5d-8605-71260bbecc25.png'\n",
        "config_yml = ''\n",
        "model_weight ='/content/drive/MyDrive/BANGLAOCR/weights/yolo/best.pt'\n",
        "\n",
        "start_time = time.time()\n",
        "result = yolo(model_weight,image_path)\n",
        "end_time = time.time()\n",
        "print(\"execution time for yolo= \",end_time - start_time)\n",
        "croped_texts = crop_all_text_box( image_path, result)\n",
        "\n",
        "start_time = time.time()\n",
        "result_lines = line_parallel_batch_processing(croped_texts, 4)\n",
        "end_time = time.time()\n",
        "print(\"execution time line detection= \",end_time - start_time)\n",
        "\n",
        "# result_word = word_segmentation(result_lines)\n",
        "# words, crops, boxes = recognize_word(result_lines, result_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wo1oM3MhyaZw"
      },
      "outputs": [],
      "source": [
        "print(len(croped_texts))\n",
        "len(result_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RIk_N9nh0cuy"
      },
      "outputs": [],
      "source": [
        "for text in croped_texts:\n",
        "    cv2_imshow(text)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KM7SIdz2ZHSe"
      },
      "outputs": [],
      "source": [
        "def crop_single_line(image, res):\n",
        "    # words[0][1]:words[0][3], words[0][0]:words[0][2]\n",
        "    x = int(res[0][0])\n",
        "    y = int(res[0][1])\n",
        "    width = int(res[0][2] - res[0][0])\n",
        "    height = int(res[0][3] - res[0][1])\n",
        "\n",
        "    crop_img = image[y:y+height, x:x+width]\n",
        "    # croped_imgs.append(crop_img)\n",
        "    # cv2_imshow(crop_img)\n",
        "\n",
        "    return crop_img\n",
        "\n",
        "\n",
        "for i in range(len(result_lines)):\n",
        "    for j in range(len(result_lines[i][0])):\n",
        "        [[x_min, y_min], [x_max, y_min], [x_max, y_max], [x_min, y_max]] = result_lines[i][0][j]\n",
        "        words= []\n",
        "        # words.append([math.floor(x_min), math.floor(y_min), math.ceil(x_max), math.ceil(y_max)])\n",
        "        words.append([math.floor(x_min), math.floor(y_min), math.ceil(x_max), math.ceil(y_max)])\n",
        "\n",
        "        # cropped_line_region = croped_texts[i][ words[0][1]:words[0][3], words[0][0]:words[0][2]]\n",
        "        cropped_line_region = crop_single_line(croped_texts[i], words)\n",
        "        cv2_imshow(cropped_line_region)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DrJoLlp8thCO"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from shapely.geometry import box\n",
        "import multiprocessing\n",
        "import threading\n",
        "config_yml = ''\n",
        "\n",
        "image = cv2.imread(image_path)\n",
        "file_name, extension = image_path.split('/')[-1].split(\".\")\n",
        "print(file_name, extension)\n",
        "\n",
        "def get_normalized_coordinates(xyxy_tensor, height, width):\n",
        "    x_min = xyxy_tensor[0][0].item() / width\n",
        "    y_min = xyxy_tensor[0][1].item() / height\n",
        "    x_max = xyxy_tensor[0][2].item() / width\n",
        "    y_max = xyxy_tensor[0][3].item() / height\n",
        "\n",
        "    coordinates = [x_min, y_min, x_max, y_max]\n",
        "    return coordinates\n",
        "\n",
        "\n",
        "def get_original_coordinates(normalized_coordinates, image_width, image_height):\n",
        "    orig_coordinates = [None]*4\n",
        "\n",
        "    orig_coordinates[0] = math.floor(normalized_coordinates[0] * image_width)\n",
        "    orig_coordinates[1] = math.floor(normalized_coordinates[1] * image_height)\n",
        "    orig_coordinates[2] = math.ceil(normalized_coordinates[2] * image_width)\n",
        "    orig_coordinates[3] = math.ceil(normalized_coordinates[3] * image_height)\n",
        "\n",
        "    return orig_coordinates\n",
        "\n",
        "\n",
        "def get_coordinates_from_segmentation(result_word):\n",
        "    words_xyxy = []\n",
        "\n",
        "    for i in range(len(result_word[0])):\n",
        "        [[x_min, y_min], [x_max, y_min], [x_max, y_max], [x_min, y_max]] = result_word[0][i]\n",
        "        words_xyxy.append([math.floor(x_min), math.floor(y_min), math.ceil(x_max), math.ceil(y_max)])\n",
        "\n",
        "    return words_xyxy\n",
        "\n",
        "\n",
        "def create_polygon(x_min, y_min, x_max, y_max):\n",
        "    pass\n",
        "\n",
        "\n",
        "names = {0: 'paragraph', 1: 'text_box', 2: 'image', 3: 'table'}\n",
        "\n",
        "def run_yolo_model(model_weight,image_path):\n",
        "    model = YOLO(model_weight)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # upscaling 2x\n",
        "    # height, width = image.shape[:2]\n",
        "    # image = cv2.resize(image, (2*width, 2*height))\n",
        "\n",
        "    # plt.imshow(image)\n",
        "    color_map = {\n",
        "        'text_box':   'red',\n",
        "        'paragraph':  'blue',\n",
        "        'image':   'green',\n",
        "        'table':  'yellow',\n",
        "    }\n",
        "\n",
        "    # layout_predicted = model(image)\n",
        "    res = model(image)[0]\n",
        "    return res\n",
        "\n",
        "\n",
        "\n",
        "def line_and_word_recognition(res, image_path):\n",
        "\n",
        "    region_of_interests = []\n",
        "    # rathern than loading the image again save the imageis in a variable\n",
        "    image = cv2.imread(image_path)\n",
        "    croped_texts= crop_all_text_box(image_path, res)\n",
        "    result_lines = line_segmentation(croped_texts)\n",
        "    result_word = word_segmentation(result_lines)\n",
        "    words, crops, boxes = recognize_word(result_lines, result_word)\n",
        "\n",
        "\n",
        "            text = []\n",
        "            # font_size = 0\n",
        "\n",
        "            for i in range(len(sorted_line_coordinates)):\n",
        "\n",
        "                    result_word = word_segmentation(cropped_line_region)\n",
        "\n",
        "                        words, crops, boxes = recognize_word(cropped_line_region, [sorted_result_word])\n",
        "\n",
        "                        text += words\n",
        "\n",
        "    return region_of_interests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HJf8Pdo2vY0Z"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "result = run_yolo_model(model_weight,image_path)\n",
        "end_time = time.time()\n",
        "print(\"execution time for layout = \"+str(end_time - start_time) + \" seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xWqwA_zVbBbQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "region_of_interests = parallel_batch_processing(result.boxes, 4)\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"execution time for line and word recognition = \"+str(end_time - start_time) + \" seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8LAlO65C8QwJ"
      },
      "outputs": [],
      "source": [
        "region_of_interests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N984aBPNZkJL"
      },
      "outputs": [],
      "source": [
        "\n",
        "start_time = time.time()\n",
        "roi = handle_overlapping_text(region_of_interests)\n",
        "end_time = time.time()\n",
        "print(\"execution time for layout postprocess= \"+str(end_time - start_time) + \" seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0bqaoZbG-s-q"
      },
      "outputs": [],
      "source": [
        "# start_time = time.time()\n",
        "# region_of_interests = line_and_word_recognition(batches[0], image_path)\n",
        "# region_of_interests = line_and_word_recognition(batches[1], image_path)\n",
        "# region_of_interests = line_and_word_recognition(batches[2], image_path)\n",
        "# region_of_interests = line_and_word_recognition(batches[3], image_path)\n",
        "# end_time = time.time()\n",
        "# print(\"execution time for line and word recognition = \"+str(end_time - start_time) + \" seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0il5qOSmqiUe"
      },
      "outputs": [],
      "source": [
        "# from multiprocessing.pool import ThreadPool as Pool\n",
        "\n",
        "# def parallel_batch_processing(boxes, num_batches):\n",
        "#     # multiprocessing.set_start_method('spawn',force=True)\n",
        "#     pool = Pool(processes=num_batches)\n",
        "#     batch_size = len(boxes) // num_batches\n",
        "#     # Split the boxes into batches\n",
        "#     batches = [boxes[i:i+batch_size] for i in range(0, len(boxes), batch_size)]\n",
        "\n",
        "\n",
        "#     # pool.map(line_and_word_recognition(result, image_path), batches)\n",
        "#     pool.map(line_and_word_recognition,[(batch, image_path) for batch in batches])\n",
        "#     pool.close()\n",
        "#     pool.join()\n",
        "#     # Create and start a thread for each batch\n",
        "\n",
        "\n",
        "\n",
        "# start_time = time.time()\n",
        "\n",
        "# parallel_batch_processing(result.boxes, 8)\n",
        "\n",
        "# end_time = time.time()\n",
        "# print(\"execution time for line and word recognition = \"+str(end_time - start_time) + \" seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Qbly6xi0t8VG"
      },
      "outputs": [],
      "source": [
        "# from joblib import Parallel, delayed\n",
        "\n",
        "\n",
        "# def parallel_batch_processing(boxes, num_batches):\n",
        "\n",
        "#     batch_size = len(boxes) // num_batches\n",
        "#     batches = [boxes[i:i+batch_size] for i in range(0, len(boxes), batch_size)]\n",
        "#     # Split the boxes into batches\n",
        "#     resutls = Parallel(n_jobs=num_batches)(delayed(line_and_word_recognition)(batch,image_path) for batch in batches)\n",
        "#     return resutls\n",
        "\n",
        "\n",
        "# start_time = time.time()\n",
        "\n",
        "# parallel_batch_processing(result.boxes, 8)\n",
        "\n",
        "# end_time = time.time()\n",
        "# print(\"execution time for line and word recognition = \"+str(end_time - start_time) + \" seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KtPv37HJzKt3"
      },
      "outputs": [],
      "source": [
        "# !pip install ray\n",
        "# import ray\n",
        "\n",
        "# def parallel_batch_processing(boxes, num_batches):\n",
        "\n",
        "#     batch_size = len(boxes) // num_batches\n",
        "#     batches = [boxes[i:i+batch_size] for i in range(0, len(boxes), batch_size)]\n",
        "#     # Initialize Ray\n",
        "#     ray.init(num_cpus=num_batches)\n",
        "\n",
        "#     # Process the batches in parallel using Ray\n",
        "#     res = ray.get([line_and_word_recognition(batch, image_path) for batch in batches])\n",
        "\n",
        "#     # Shut down Ray\n",
        "#     ray.shutdown()\n",
        "\n",
        "#     return res\n",
        "\n",
        "\n",
        "# # start_time = time.time()\n",
        "\n",
        "# parallel_batch_processing(result.boxes, 8)\n",
        "\n",
        "# # end_time = time.time()\n",
        "# # print(\"execution time for line and word recognition = \"+str(end_time - start_time) + \" seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lNW6lL8a2-Gn"
      },
      "outputs": [],
      "source": [
        "# import dask\n",
        "# import dask.bag as db\n",
        "\n",
        "# def parallel_batch_processing(boxes, num_batches):\n",
        "\n",
        "#     batch_size = len(boxes) // num_batches\n",
        "#     batches = [boxes[i:i+batch_size] for i in range(0, len(boxes), batch_size)]\n",
        "#    # Create a Dask bag from the batches\n",
        "#     bag = db.from_sequence(batches)\n",
        "\n",
        "#     # Process the batches in parallel using Dask\n",
        "#     # bag.map(process_batch).compute(num_workers=num_batches)\n",
        "#     results = bag.map(line_and_word_recognition, arg1=batches, arg2=image_path).compute()\n",
        "\n",
        "#     return res\n",
        "\n",
        "\n",
        "# # start_time = time.time()\n",
        "\n",
        "# parallel_batch_processing(result.boxes, 8)\n",
        "\n",
        "# # end_time = time.time()\n",
        "# # print(\"execution time for li"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}